{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals of Deep Learning\n",
    "## 目录\n",
    "- Chapter 4. Beyond Gradient Descent\n",
    "    - Local Minima in the Error Surfaces of Deep Networks\n",
    "    - How Pesky Are Spurious Local Minima in Deep Networks?\n",
    "    - Flat Regions in the Error Surface\n",
    "    - When the Gradient Points in the Wrong Direction\n",
    "    - Momentum-Based Optimization\n",
    "    - A Brief View of Second-Order Methods\n",
    "        - Conjugate Gradient Descent\n",
    "        - Broyden–Fletcher–Goldfarb–Shanno (BFGS)\n",
    "    - Learning Rate Adaptation\n",
    "        - AdaGrad—Accumulating Historical Gradients\n",
    "        - RMSProp—Exponentially Weighted Moving Average of Gradients\n",
    "        - Adam—Combining Momentum and RMSProp\n",
    "    - Optimization Algorithms Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4. Beyond Gradient Descent\n",
    "## Local Minima in the Error Surfaces of Deep Networks\n",
    "The primary challenge in optimizing deep learning models is that **we are forced to use minimal local information to infer the global structure of the error surface.** This is a hard problem because there is usually very little correspondence between local and global structure.\n",
    "\n",
    "In Chapter 2, we talked about how a mini-batch version of gradient descent can help navigate a troublesome error surface when there are spurious regions of magnitude zero gradients. But as we can see in Figure 4-1, even a stochastic error surface won’t save us from a deep local minimum.\n",
    "\n",
    "![4-1](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0401.png)\n",
    "\n",
    "Figure 4-1. Mini-batch gradient descent may aid in escaping shallow local minima, but often fails when dealing with deep local minima, as shown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Pesky Are Spurious Local Minima in Deep Networks?\n",
    "`Goodfellow et al.`:Instead of analyzing the error function over time, they cleverly investigated what happens on the error surface between a randomly initialized parameter vector and a successful final solution by using linear interpolation. So given a randomly initialized parameter vector $\\theta_i$ and stochastic gradient descent (SGD) solution $\\theta_f$, we aim to compute the error function at every point along the linear interpolation $\\theta_\\alpha=\\alpha \\cdot \\theta_f + (1-\\alpha) \\cdot \\theta_i$\n",
    "\n",
    "In other words, they wanted to investigate whether local minima would hinder our gradient-based search method even if we knew which direction to move in. They showed that **for a wide variety of practical networks with different types of neurons, the direct path between a randomly initialized point in the parameter space and a stochastic gradient descent solution isn’t plagued with troublesome local minima.**\n",
    "\n",
    "We can even demonstrate this ourselves using the feed-foward ReLU network we built in Chapter 3. \n",
    "\n",
    "1. Using a checkpoint file that we saved while training our original feed-forward network, we can re-instantiate the inference and loss components while also maintaining a list of pointers to the variables in the original graph for future use in. `var_list_opt` (where opt stands for the optimal parameter settings)\n",
    "1. Similarly, we can reuse the component constructors to create a randomly initialized network. Here we store the variables in `var_list_rand` for the next step of our program.\n",
    "1. With these two networks appropriately initialized, we can now construct the linear interpolation using the mixing parameters `alpha` and `beta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n",
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n",
      "INFO:tensorflow:Restoring parameters from mlp_logs/model-checkpoint-33000\n",
      "[0.080450691, 2.382179]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import input_data\n",
    "mnist = input_data.read_data_sets(\"data/\", one_hot=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from multilayer_perceptron import inference, loss\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "x = tf.placeholder(\"float\", [None, 784]) # mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(\"float\", [None, 10]) # 0-9 digits recognition => 10 classes\n",
    "\n",
    "with tf.variable_scope(\"mlp_model\") as scope:\n",
    "    output_opt = inference(x)\n",
    "    cost_opt = loss(output_opt, y)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    scope.reuse_variables()\n",
    "    \n",
    "    var_list_opt = [\"hidden_1/W\", \"hidden_1/b\", \"hidden_2/W\", \"hidden_2/b\", \"output/W\", \"output/b\"]\n",
    "    var_list_opt = [tf.get_variable(v) for v in var_list_opt]\n",
    "\n",
    "    saver.restore(sess, \"mlp_logs/model-checkpoint-33000\")\n",
    "\n",
    "with tf.variable_scope(\"mlp_init\") as scope:\n",
    "    output_rand = inference(x)\n",
    "    cost_rand = loss(output_rand, y)\n",
    "\n",
    "    scope.reuse_variables()\n",
    "\n",
    "    var_list_rand = [\"hidden_1/W\", \"hidden_1/b\", \"hidden_2/W\", \"hidden_2/b\", \"output/W\", \"output/b\"]\n",
    "    var_list_rand = [tf.get_variable(v) for v in var_list_rand]\n",
    "\n",
    "    init_op = tf.variables_initializer(var_list_rand)\n",
    "\n",
    "    sess.run(init_op)\n",
    "\n",
    "feed_dict = {\n",
    "    x: mnist.test.images,\n",
    "    y: mnist.test.labels,\n",
    "}\n",
    "\n",
    "print sess.run([cost_opt, cost_rand], feed_dict=feed_dict)\n",
    "\n",
    "with tf.variable_scope(\"mlp_inter\") as scope:\n",
    "    alpha = tf.placeholder(\"float\", [1, 1])\n",
    "\n",
    "    h1_W_inter = var_list_opt[0] * (1 - alpha) + var_list_rand[0] * (alpha)\n",
    "    h1_b_inter = var_list_opt[1] * (1 - alpha) + var_list_rand[1] * (alpha)\n",
    "    h2_W_inter = var_list_opt[2] * (1 - alpha) + var_list_rand[2] * (alpha)\n",
    "    h2_b_inter = var_list_opt[3] * (1 - alpha) + var_list_rand[3] * (alpha)\n",
    "    o_W_inter = var_list_opt[4] * (1 - alpha) + var_list_rand[4] * (alpha)\n",
    "    o_b_inter = var_list_opt[5] * (1 - alpha) + var_list_rand[5] * (alpha)\n",
    "\n",
    "    h1_inter = tf.nn.relu(tf.matmul(x, h1_W_inter) + h1_b_inter)\n",
    "    h2_inter = tf.nn.relu(tf.matmul(h1_inter, h2_W_inter) + h2_b_inter)\n",
    "    o_inter = tf.nn.relu(tf.matmul(h2_inter, o_W_inter) + o_b_inter)\n",
    "\n",
    "    cost_inter = loss(o_inter, y)\n",
    "    tf.summary.scalar(\"interpolated_cost\", cost_inter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can vary the value of `alpha` to understand how the error surface changes as we traverse the line between the randomly initialized point and the final SGD solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEPCAYAAABFpK+YAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x104571290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "summary_writer = tf.summary.FileWriter(\"linear_interp_logs/\", graph=sess.graph)\n",
    "summary_op = tf.summary.merge_all()\n",
    "results = []\n",
    "for a in np.arange(-2, 2, 0.01):\n",
    "    feed_dict = {\n",
    "        x: mnist.test.images,\n",
    "        y: mnist.test.labels,\n",
    "        alpha: [[a]],\n",
    "    }\n",
    "\n",
    "    cost, summary_str = sess.run([cost_inter, summary_op], feed_dict=feed_dict)\n",
    "    summary_writer.add_summary(summary_str, (a + 2)/0.01)\n",
    "    results.append(cost)\n",
    "\n",
    "plt.plot(np.arange(-2, 2, 0.01), results, 'ro')\n",
    "plt.ylabel('Incurred Error')\n",
    "plt.xlabel('Alpha')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The cost function of a three-layer feed-forward network as we linearly interpolate on the line connecting a randomly initialized parameter vector and an SGD solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flat Regions in the Error Surface\n",
    "More generally, given an arbitrary function, a point at which the gradient is the zero vector is called a `critical point`.\n",
    "\n",
    "For a one-dimensional cost function, a critical point can take one of three forms, as shown in Figure 4-4.\n",
    "\n",
    "![4-4](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0404.png)\n",
    "\n",
    "Figure 4-4. Analyzing a critical point along a single dimension\n",
    "\n",
    "This means given a random critical point in a random one-dimensional function, it has one-third probability of being a local minimum. This means that if we have a total of k critical points, we can expect to have a total of $\\frac{k}{3}$  local minima.\n",
    "\n",
    "We can also extend this to higher dimensional functions. Consider a cost function operating in a d-dimensional space. \n",
    "\n",
    "![4-5](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0405.png)\n",
    "\n",
    "Figure 4-5. A saddle point over a two-dimensional error surface\n",
    "\n",
    "In general, in a d-dimensional parameter space, we can slice through a critical point on d different axes. **A critical point can only be a local minimum if it appears as a local minimum in every single one of the d one-dimensional subspaces.** Using the fact that a critical point can come in one of three different flavors(local minima,local maxima and saddle points) in a one-dimensional subspace, we realize that the probability that a random critical point is in a random function is $\\frac{1}{3^d}$. This means that a random function function with k critical points has an expected number of $\\frac{k}{3^d}$ local minima. In other words, **as the dimensionality of our parameter space increases, local minima become exponentially more rare.**\n",
    "\n",
    "It seems **like these flat segments of the error surface are pesky but ultimately don’t prevent stochastic gradient descent from converging to a good answer.** However, it does pose serious problems for methods that attempt to directly solve for a point where the gradient is zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When the Gradient Points in the Wrong Direction\n",
    "As an example, we consider an error surface defined over a two-dimensional parameter space, as shown in Figure 4-6.\n",
    "\n",
    "![4-6](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0406.png)\n",
    "\n",
    "Figure 4-6. Local information encoded by the gradient usually does not corroborate the global structure of the error surface\n",
    "\n",
    "Specifically, we realize that only when the contours are perfectly circular does the gradient always point in the direction of the local minimum. However, if the contours are extremely elliptical (as is usually the case for the error surfaces of deep networks), the gradient can be as inaccurate as 90 degrees away from the correct direction!\n",
    "\n",
    "The general problem with taking a significant step in this direction, however, is that the gradient could be changing under our feet as we move! We demonstrate this simple fact in Figure 4-7. Going back to the two-dimensional example, if our contours are perfectly circular and we take a big step in the direction of the steepest descent, the gradient doesn’t change direction as we move. However, this is not the case for highly elliptical contours.\n",
    "\n",
    "![4-7](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0407.png)\n",
    "\n",
    "Figure 4-7. We show how the direction of the gradient changes as we move along the direction of steepest descent (as determined from a starting point). The gradient vectors are normalized to identical length to emphasize the change in direction of the gradient vector. \n",
    "\n",
    "More generally, we can quantify how the gradient changes under our feet as we move in a certain direction by computing second derivatives. Specifically, we want to measure $\\frac{\\partial(\\partial E / \\partial w_j)}{\\partial w_j}$, which tells us how the gradient component for $w_j$ changes as we change the value of $w_i$. We can compile this information into a special matrix known as the `Hessian matrix` (H). And when describing an error surface where the gradient changes underneath our feet as we move in the direction of steepest descent, this matrix is said to be `ill-conditioned`.\n",
    "\n",
    "We can now use a second-order approximation via Taylor series:\n",
    "\n",
    "$$E(x) \\approx E(x^{(i)}) + (x-x^{(i)})^T g + \\frac{1}{2} (x-x^{(i)})^T H (x-x^{(i)})$$\n",
    "\n",
    "If we go further to state that we will be moving $\\epsilon$ units in the direction of the gradient, we can further simplify our expression:\n",
    "\n",
    "$$E(x^{(i)} - \\epsilon g) \\approx E(x^{(i)}) - \\epsilon g^T g + \\frac{1}{2}\\epsilon^2 g^THg$$\n",
    "\n",
    "This expression consists of three terms: \n",
    "\n",
    "1. the value of the error function at the original parameter vector, \n",
    "1. the improvement in error afforded by the magnitude of the gradient, \n",
    "1. a correction term that incorporates the curvature of the surface as represented by the Hessian matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Momentum-Based Optimization\n",
    "Fundamentally, **the problem of an ill-conditioned Hessian matrix manifests itself in the form of gradients that fluctuate wildly.** As a result, one popular mechanism for dealing with ill-conditioning bypasses the computation of the Hessian, and instead, focuses on how to cancel out these fluctuations over the duration of training.\n",
    "\n",
    "Our goal, then, is to somehow generate an analog for velocity in our optimization algorithm. We can do this by keeping track of an exponentially weighted decay of past gradients. The premise is simple: every update is computed by combining the update in the last iteration with the current gradient. Concretely, we compute the change in the parameter vector as follows:\n",
    "\n",
    "$$v_i = mv_{i-1} - \\epsilon g_i$$\n",
    "\n",
    "$$\\theta_i = \\theta_{i-1} + v_i$$\n",
    "\n",
    "In other words, we use the  momentum hyperparameter m to determine what fraction of the previous velocity to retain in the new update, and add this “memory” of past gradients to our current gradient. This approach is commonly referred to as `momentum`.\n",
    "\n",
    "To better visualize how momentum works, we’ll explore a toy example. Specifically, we’ll investigate how momentum affects updates during a `random walk`. A random walk is a succession of randomly chosen steps. In our example, we’ll imagine a particle on a line that, at every time interval, randomly picks a step size between -10 and 10 and takes a moves in that direction. This is simply expressed as:\n",
    "\n",
    "```py\n",
    "step_range = 10\n",
    "step_choices = range(-1 * step_range, step_range + 1)\n",
    "rand_walk = [random.choice(step_choices) for x in xrange(100)]\n",
    "```\n",
    "\n",
    "We’ll then simulate what happens when we use a slight modification of momentum (i.e., the standard exponentially weighted moving average algorithm) to smooth our choice of step at every time interval. Again, we can concisely express this as:\n",
    "\n",
    "```py\n",
    "momentum_rand_walk = [random.choice(step_choices)]\n",
    "for i in xrange(len(rand_walk) - 1):\n",
    "    prev = momentum_rand_walk[-1]\n",
    "    rand_choice = random.choice(step_choices)\n",
    "    new_step = momentum * prev + (1 - momentum) * rand_choice\n",
    "    momentum_rand_walk.append()\n",
    "```\n",
    "\n",
    "![4-8](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0408.png)\n",
    "\n",
    "Figure 4-8. Momentum smooths volatility in the step sizes during a random walk using an exponentially weighted moving average\n",
    "\n",
    "The resulting speedup is staggering. We display how the cost function changes over time by comparing the TensorBoard visualizations in Figure 4-9. The figure demonstrates that to achieve a cost of 0.1 without momentum (right) requires nearly 18,000 steps (minibatches), whereas with momentum (left), we require just over 2,000.\n",
    "\n",
    "![4-9](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0409.png)\n",
    "\n",
    "Figure 4-9. Comparing training a feed-forward network with (right) and without (left) momentum demonstrates a massive decrease in training time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Brief View of Second-Order Methods\n",
    "Several second-order methods, however, have been researched over the past several years that attempt to approximate the Hessian directly.\n",
    "\n",
    "### Conjugate Gradient Descent\n",
    "In steepest descent, we compute the direction of the gradient and then `line search` to find the minimum along that direction. We jump to the minimum and then recompute the gradient to determine the direction of the next line search. It turns out that this method ends up zigzagging a significant amount, as shown in Figure 4-9, because each time we move in the direction of steepest descent, we undo a little bit of progress in another direction. A remedy to this problem is moving in a `conjugate direction` relative to the previous choice instead of the direction of steepest descent. The conjugate direction is chosen by using an indirect approximation of the Hessian to linearly combine the gradient and our previous direction. With a slight modification, this method generalizes to the nonconvex error surfaces we find in deep networks.\n",
    "\n",
    "![4-10](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0410.png)\n",
    "\n",
    "Figure 4-10. The method of steepest descent often zigzags; conjugate descent attempts to remedy this issue\n",
    "\n",
    "### Broyden–Fletcher–Goldfarb–Shanno (BFGS)\n",
    "`BFGS` algorithm attempts to compute the inverse of the Hessian matrix iteratively and use the inverse Hessian to more effectively optimize the parameter vector. A more memory-efficient version known as `L-BFGS`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Adaptation\n",
    "### AdaGrad—Accumulating Historical Gradients\n",
    "This learning rate is inversely scaled with respect to the square root of the sum of the squares (root mean square) of all the parameter’s historical gradients.\n",
    "\n",
    "Flat regions may force AdaGrad to decrease the learning rate before it reaches a minimum.\n",
    "\n",
    "$$r_i=r_{i-1} + g_i \\odot g_i$$\n",
    "\n",
    "- $r_0=0$\n",
    "- $\\odot$ is element-wise tensor multiplication\n",
    "- $g_i \\odot g_i$ the square of all the gradient parameters\n",
    "\n",
    "$$\\theta_i = \\theta_{i-1} - \\frac{\\epsilon}{\\sigma \\oplus \\sqrt{r_i}} \\odot g$$\n",
    "\n",
    "- $\\epsilon$ global learning rate, is divided by the square root of the gradient accumulation vector\n",
    "- $\\sigma(\\approx 10^{-7})$ ,a tiny number in order to prevent division by zero\n",
    "\n",
    "In TensorFlow, a built-in optimizer allows for easily utilizing AdaGrad as a learning algorithm:\n",
    "\n",
    "```py\n",
    "tf.train.AdagradOptimizer(learning_rate, \n",
    "                          initial_accumulator_value=0.1, \n",
    "                          use_locking=False, \n",
    "                          name='Adagrad')\n",
    "```\n",
    "\n",
    "- the $\\epsilon$ and initial gradient accumulation vector are rolled together into the `initial_accumulator_value` argument\n",
    "\n",
    "### RMSProp—Exponentially Weighted Moving Average of Gradients\n",
    "$$r_i=\\rho r_{i-1} + (1 - \\rho) g_i \\odot g_i$$\n",
    "\n",
    "- $\\rho$ the decay factor determines how long we keep old gradients\n",
    "\n",
    "```py\n",
    "tf.train.RMSPropOptimizer(learning_rate, decay=0.9, \n",
    "                          momentum=0.0, epsilon=1e-10, \n",
    "                          use_locking=False, name='RMSProp')\n",
    "```\n",
    "\n",
    "- Unlike in Adagrad, we pass in $\\delta$ separately as the epsilon argument to the constructor.\n",
    "\n",
    "### Adam—Combining Momentum and RMSProp\n",
    "$$\\tilde{m_i} = \\frac{m_i}{1-\\beta_1^i}$$\n",
    "\n",
    "$$v_i=\\frac{v_i}{1-\\beta_2^i}$$\n",
    "\n",
    "$$\\theta_i=\\theta_{i-1} - \\frac{\\epsilon}{\\sigma \\oplus \\sqrt{v_i}\\tilde{m_i}}$$\n",
    "\n",
    "```py\n",
    "tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.9, \n",
    "                       beta2=0.999, epsilon=1e-08, \n",
    "                       use_locking=False, name='Adam')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Algorithms Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n",
      "sgd\n",
      "2017-08-15 08:28:09.183430: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-08-15 08:28:09.183453: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-08-15 08:28:09.183459: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-08-15 08:28:09.183464: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-08-15 08:28:09.183469: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "Epoch: 0001 cost = 1.023527871\n",
      "Validation Error: 0.119799971581\n",
      "Epoch: 0101 cost = 0.018873566\n",
      "Validation Error: 0.019200026989\n",
      "Epoch: 0201 cost = 0.005179930\n",
      "Validation Error: 0.0184000134468\n",
      "Epoch: 0301 cost = 0.002607897\n",
      "Validation Error: 0.0171999931335\n",
      "Epoch: 0401 cost = 0.001667035\n",
      "Validation Error: 0.0166000127792\n",
      "Optimization Finished!\n",
      "Test Accuracy: 0.9807\n"
     ]
    }
   ],
   "source": [
    "!python optimzer_mlp.py sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n",
      "momentum\n",
      "2017-08-15 09:07:09.048477: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-08-15 09:07:09.048500: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-08-15 09:07:09.048506: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-08-15 09:07:09.048511: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "2017-08-15 09:07:09.048515: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
      "Epoch: 0001 cost = 0.919267848\n",
      "Validation Error: 0.270600020885\n",
      "Epoch: 0101 cost = 0.260265864\n",
      "Validation Error: 0.128000020981\n",
      "Epoch: 0201 cost = 0.260049450\n",
      "Validation Error: 0.127799987793\n",
      "Epoch: 0301 cost = 0.260005093\n",
      "Validation Error: 0.128199994564\n",
      "Epoch: 0401 cost = 0.259985650\n",
      "Validation Error: 0.127799987793\n",
      "Optimization Finished!\n",
      "Test Accuracy: 0.8683\n"
     ]
    }
   ],
   "source": [
    "!python optimzer_mlp.py momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

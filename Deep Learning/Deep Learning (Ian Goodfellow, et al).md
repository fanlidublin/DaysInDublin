
# Deep Learning
**Ian Goodfellow, Yoshua Bengio and Aaron Courville**

![](http://t2.gstatic.com/images?q=tbn:ANd9GcRq9yUJmITp5UfjgEIHlsAi34WXhk8P13CCQoKjIRqleVZQI8uq)

## Contents 目录
#### 1. Introduction
    1.1 Who Should Read This Book?  本书面向的读者
    1.2 Historicak Trends in Deep Learning  深度学习的历史趋势

### <font color="#0000CC">*Applied Math and Machine Learning Basics*</font>
#### 2. Linear Algebra
    2.1 Scalars, Vectors, Matrices and Tensors  标量、向量、矩阵和张量
    2.2 Multiplying Matrices and Vectors  矩阵与向量相乘
    2.3 Identity and Inverse Matrices  单位矩阵与逆矩阵
    2.4 Linear Dependence and Span  线性相关与生成子空间
    2.5 Norms  范数
    2.6 Special Kinds of Matrices and Vectors  特殊类型的矩阵和向量
    2.7 Eigendecpmposition  特征分解
    2.8 Singular Value Decomposition  奇异值分解
    2.9 The Moore-Penrose Pseudoinverse  Moore-Penrose 伪逆
    2.10 The Trace Operator  逆运算
    2.11 The Determinant  行列式
    2.12 Example: Principal Components Analysis (PCA)  实例：主成分分析

#### 3. Probability and Information Theory
    3.1 Why Probability?  使用概率的原因
    3.2 Random Variables  随机变量
    3.3 Probability Distributions  概率分布
        3.3.1 Discrete Variables and Probability Mass Functions  离散型变量和概率质量函数
        3.3.2 Continuous Variables and Probability Density Function  连续型变量和概率密度函数
    3.4 Marginal Probability  边缘概率
    3.5 Conditional Probabiliyu  条件概率
    3.6 The Chain Rule of Conditional Probabilities  条件概率的链式法则
    3.7 Independence and Conditional Independence  独立性和条件独立性
    3.8 Expectation, Variance and Covariance  期望、方差和协方差
    3.9 Common Probability Distributions  常用概率分布
        3.9.1 Bernoulli Distribution
        3.9.2 Multinoulli Distribution
        3.9.3 Gaussian Distribution
        3.9.4 Exponential and Laplace Distributions
        3.9.5 The Dirac Distribution and Empirical Distribution
        3.9.6 Mixtures of Distributions
    3.10 Useful Properties of Common Functions
    3.11 Bayes'Rule
    3.12 Technical Details of Continuous Variables
    3.13 Information Theory
    3.14 Structured Probabilitic Models

#### 4. Numerical Computions
    4.1 Overflow and Uderflow
    4.2 Poor Conditioning
    4.3 Gradient-Based Optimization
    4.4 Constrained Optimization
    4.5 Example: Linear Least Squares

#### 5. Machine Learning Basics
    5.1 Learning Algorithms
    5.2 Capacity, Overfitting and Underfitting
    5.3 Hyperparameters and Validation Sets
    5.4 Estimators, Bias and Variance
    5.5 Maximum Likelihood Estimation
    5.6 Bayesian Statistics
    5.7 Supervised Learning Algorithms
    5.8 Unsupervised Learning Algorithms
    5.9 Stochastic Gradient Descent
    5.10 Building a Machine Learning Algorithm
    5.11 Challenges Motivating Deep Learning

### <font color="#0000CC">*Deep Networks: Modern Practices*</font>
#### 6. Deep Feedforward Networks
    6.1 Example: Learning XOR
    6.2 Gradient-Based Learning
    6.3 Hidden Units
    6.4 Architecture Design
    6.5 Back-Propagation and Other Differentiation Algorithms
    6.6 Historical Notes

#### 7. Regularization for Deep Learning
    7.1 Parameter Norm Penalties
    7.2 Norm Penalties as Constrained Optimization
    7.3 Regularization and Under-Constrained Problems
    7.4 Dataset Augmentation
    7.5 Noise Robustness
    7.6 Semi-Supervised Learning
    7.7 Multitask Learning
    7.8 Early Stopping
    7.9 Parameter Tying and Parameter Sharing
    7.10 Sparse Representations
    7.11 Bagging and Other Ensemble Methods
    7.12 Dropout
    7.13 Adversarial Training
    7.14 Tangent Distance, Tangent Prop and Manifold Tangent Classifier

#### 8. Optimization for Training Deep Models
    8.1 How Learning Differs from Pure Optimization
    8.2 Challenges in Neural Network Optimization
    8.3 Basic Algorithms
    8.4 Parameter Initialization Strategies
    8.5 Algorithms with Adaptive Learning Rates
    8.6 Approximate Second-Order Methods
    8.7 Optimization Strategies and Meta-Algorithms

#### 9. Convolutional Networks
    9.1



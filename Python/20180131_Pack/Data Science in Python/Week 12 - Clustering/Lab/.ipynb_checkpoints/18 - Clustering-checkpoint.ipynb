{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Clustering* in an unsupervised machine learning task, where an algorithm is used to organise data into groups in the absence of any external information (e.g. class labels), only relying on the data itself (e.g. feature values and often some similarity/distance value computed on those features).\n",
    "\n",
    "*Partitional clustering*: Algorithms which attempt to directly decompose a data set into a “flat” grouping consisting of a number of disjoint (non-overlapping) clusters. Usually pre-specify number of clusters *k*. \n",
    "\n",
    "The best known example of a partitional clustering algorithm is *k-means*. This algorithm searches for cluster centroids which are the mean of the points within them, such that every item is closest to the cluster centroid it is assigned to. Items are repeatedly re-assigned until the algorithm converges to a solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitional Example 1: Artificial Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's try creating an artificial dataset with 200 items which we will use to test out partitional clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "X, y = make_blobs(n_samples=200, centers=4, random_state=0, cluster_std=0.60)\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is stored in X. Note that the array y contains the \"true\" clustering of the data - this information is not available to the clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# Note: the 's' parameter controls the size of the points\n",
    "plt.scatter(X[:, 0], X[:, 1], s=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply the k-means algorithm to automatically split the data into *k=4* clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# find 4 clusters\n",
    "model = KMeans(4)  \n",
    "model.fit(X)\n",
    "clustering = model.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then plot these clusters in different colours - we can see that the 4 clusters are well-separated in the 2D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=clustering, s=30, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add the centroids (the cluster mean vectors) to the plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=clustering, s=30, cmap='rainbow')\n",
    "centroids = model.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], marker='x', s=150, linewidths=3, color='orange', zorder=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above we can visually see that the data naturally has 4 clusters. Using a different value of k would have given us clusters that were not dense and well-separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find 2 clusters\n",
    "model = KMeans(2)  \n",
    "model.fit(X)\n",
    "clustering = model.labels_\n",
    "plt.scatter(X[:, 0], X[:, 1], c=clustering, s=30, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find 6 clusters\n",
    "model = KMeans(6)  \n",
    "model.fit(X)\n",
    "clustering = model.labels_\n",
    "plt.scatter(X[:, 0], X[:, 1], c=clustering, s=30, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitional Example 2: Iris Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our second example, we will apply k-means to the Iris dataset for *k=3* clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "data = iris.data\n",
    "target = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# find 3 clusters\n",
    "model = KMeans(3)\n",
    "model.fit(data) \n",
    "clustering = model.labels_\n",
    "print(clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Iris dataset has 3 features. We can visualise the clusters using each pair of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[:, 0], data[:, 1], c=clustering, s=50, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[:, 1], data[:, 2], c=clustering, s=50, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[:, 0], data[:, 2], c=clustering, s=50, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply k-means with *k=4* clusters would have given us a considerably different result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(4)\n",
    "model.fit(data) \n",
    "clustering = model.labels_\n",
    "plt.scatter(data[:, 0], data[:, 2], c=clustering, s=50, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of generating a flat partition of data, it can be useful to construct a hierarchy of items by producing a set of nested clusters that are arranged to form a tree structure.\n",
    "\n",
    "*Agglomerative algorithms* start with each item assigned to its own cluster, and then apply a bottom-up strategy where, at each step, the most similar (least distant) pair of clusters are merged. We continue until we reach a specified number of clusters or until every item is in a single cluster\n",
    "\n",
    "To compute the distance between each pair of items, we need some kind of *distance metric* (e.g. Euclidean distance).\n",
    "\n",
    "A range of different criteria exist for choosing which pair of clusters to merge at each step - the *linkage metric* (e.g. single linkage, average linkage...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the sckit-learn implementation of hierarchical clustering, we specifiy the number of clusters at which to cut-off the tree. This gives us a flat clustering, like k-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut off at 2 clusters\n",
    "model = AgglomerativeClustering(n_clusters=2,affinity=\"euclidean\",linkage=\"average\")\n",
    "model.fit(data) \n",
    "# get the flat clustering\n",
    "clustering = model.labels_\n",
    "print(clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualise this clustering in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[:, 0], data[:, 1], c=clustering, s=50, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering with SciPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SciPy library of data analysis routines also includes implementations of clustering algorithms, including agglomerative clustering. This implementation builds an entire tree of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create a small artificial dataset to test with..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "X, y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now build the tree..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as hac\n",
    "tree = hac.linkage(X, method=\"complete\", metric=\"euclidean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then visualise this tree as a *dendrogram*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7))\n",
    "hac.dendrogram(tree)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also rotate the tree 90 degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7))\n",
    "hac.dendrogram(tree,orientation=\"right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also cut-off the tree at some point, to produce a flat clustering. Note the cluster indices count from 1, not 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import fcluster\n",
    "# cut-off the tree to leave 2 clusters\n",
    "clustering = fcluster(tree,2,'maxclust')\n",
    "print(clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the flat clustering against the 2 dimensions of the data\n",
    "plt.scatter(X[:, 0], X[:, 1], c=clustering, s=30, cmap='rainbow')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

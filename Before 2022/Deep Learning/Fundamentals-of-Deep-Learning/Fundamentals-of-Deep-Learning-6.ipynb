{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals of Deep Learning\n",
    "## 目录\n",
    "- Chapter 6. Embedding and Representation Learning\n",
    "    - Learning Lower-Dimensional Representations\n",
    "    - Principal Component Analysis\n",
    "    - Motivating the Autoencoder Architecture\n",
    "    - Implementing an Autoencoder in TensorFlow\n",
    "    - Denoising to Force Robust Representations\n",
    "    - Sparsity in Autoencoders\n",
    "    - When Context Is More Informative than the Input Vector\n",
    "    - The Word2Vec Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Lower-Dimensional Representations\n",
    "![6-1](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0601.png)\n",
    "\n",
    "Figure 6-1. Using embeddings to automate feature selection in the face of scarce labeled data\n",
    "\n",
    "## Principal Component Analysis\n",
    "More specifically, if we have d-dimensional data, we’d like to find a new set of m < d dimensions that conserves as much valuable information from the original dataset. \n",
    "\n",
    "First we find a unit vector along which the dataset has maximum variance.  Because this direction contains the most information, we select this direction as our first axis. Then from the set of vectors orthogonal to this first choice, we pick a new unit vector along which the dataset has maximum variance. This is our second axis. We continue this process until we have found a total of d new vectors that represent new axes. We project our data onto this new set of axes.  We then decide a good value for m and toss out all but the first m axes (the principal components, which store the most information). The result is shown in Figure 6-2.\n",
    "\n",
    "![6-2](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0602.png)\n",
    "\n",
    "Figure 6-2. An illustration of PCA for dimensionality reduction to capture the dimension with the most information (as proxied by variance)\n",
    "\n",
    "While PCA has been used for decades for dimensionality reduction, it spectacularly fails to capture important relationships that are piecewise linear or nonlinear.\n",
    "\n",
    "![6-3](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0603.png)\n",
    "\n",
    "Figure 6-3. A situation in which PCA fails to optimally transform the data for dimensionality reduction\n",
    "\n",
    "## Motivating the Autoencoder Architecture\n",
    "![6-4](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0604.png)\n",
    "\n",
    "Figure 6-4. The autoencoder architecture attempts to construct a high-dimensional input into a low-dimensional embedding and then uses that low-dimensional embedding to reconstruct the input\n",
    "\n",
    "## Implementing an Autoencoder in TensorFlow\n",
    "“Reducing the dimensionality of data with neural networks”(Hinton and Salakhutdinov in 2006):Their hypothesis was that **the nonlinear complexities afforded by a neural model would allow them to capture structure that linear methods, such as PCA, would miss.**\n",
    "\n",
    "![6-5](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0605.png)\n",
    "\n",
    "Figure 6-5. The experimental setup for dimensionality reduction of the MNIST dataset employed by Hinton and Salakhutdinov, 2006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def layer_batch_norm(x, n_out, phase_train):\n",
    "    beta_init = tf.constant_initializer(value=0.0, dtype=tf.float32)\n",
    "    gamma_init = tf.constant_initializer(value=1.0, dtype=tf.float32)\n",
    "\n",
    "    beta = tf.get_variable(\"beta\", [n_out], initializer=beta_init)\n",
    "    gamma = tf.get_variable(\"gamma\", [n_out], initializer=gamma_init)\n",
    "\n",
    "    batch_mean, batch_var = tf.nn.moments(x, [0], name='moments')\n",
    "    ema = tf.train.ExponentialMovingAverage(decay=0.9)\n",
    "    ema_apply_op = ema.apply([batch_mean, batch_var])\n",
    "    ema_mean, ema_var = ema.average(batch_mean), ema.average(batch_var)\n",
    "    def mean_var_with_update():\n",
    "        with tf.control_dependencies([ema_apply_op]):\n",
    "            return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "    mean, var = tf.cond(phase_train,\n",
    "        mean_var_with_update,\n",
    "        lambda: (ema_mean, ema_var))\n",
    "\n",
    "    reshaped_x = tf.reshape(x, [-1, 1, 1, n_out])\n",
    "    normed = tf.nn.batch_norm_with_global_normalization(reshaped_x, mean, var,\n",
    "        beta, gamma, 1e-3, True)\n",
    "    return tf.reshape(normed, [-1, n_out])\n",
    "\n",
    "def layer(input, weight_shape, bias_shape, phase_train):\n",
    "    weight_init = tf.random_normal_initializer(stddev=(2.0/weight_shape[0])**0.5)\n",
    "    bias_init = tf.constant_initializer(value=0)\n",
    "    W = tf.get_variable(\"W\", weight_shape, initializer=weight_init)\n",
    "    b = tf.get_variable(\"b\", bias_shape, initializer=bias_init)\n",
    "    logits = tf.matmul(input, W) + b\n",
    "    return tf.nn.relu(layer_batch_norm(logits, weight_shape[1], phase_train))\n",
    "\n",
    "def encoder(x, phase_train):\n",
    "    with tf.variable_scope(\"encoder\"):\n",
    "        with tf.variable_scope(\"hidden_1\"):\n",
    "            hidden_1 = layer(x, [784, n_encoder_hidden_1], [n_encoder_hidden_1], phase_train)\n",
    "         \n",
    "        with tf.variable_scope(\"hidden_2\"):\n",
    "            hidden_2 = layer(hidden_1, [n_encoder_hidden_1, n_encoder_hidden_2], [n_encoder_hidden_2], phase_train)\n",
    "         \n",
    "        with tf.variable_scope(\"hidden_3\"):\n",
    "            hidden_3 = layer(hidden_2, [n_encoder_hidden_2, n_encoder_hidden_3], [n_encoder_hidden_3], phase_train)\n",
    "\n",
    "        with tf.variable_scope(\"code\"):\n",
    "            code = layer(hidden_3, [n_encoder_hidden_3, n_code], [n_code], phase_train)\n",
    "\n",
    "    return code\n",
    "\n",
    "def decoder(code, phase_train):\n",
    "    with tf.variable_scope(\"decoder\"):\n",
    "        with tf.variable_scope(\"hidden_1\"):\n",
    "            hidden_1 = layer(code, [n_code, n_decoder_hidden_1], [n_decoder_hidden_1], phase_train)\n",
    "         \n",
    "        with tf.variable_scope(\"hidden_2\"):\n",
    "            hidden_2 = layer(hidden_1, [n_decoder_hidden_1, n_decoder_hidden_2], [n_decoder_hidden_2], phase_train)\n",
    "         \n",
    "        with tf.variable_scope(\"hidden_3\"):\n",
    "            hidden_3 = layer(hidden_2, [n_decoder_hidden_2, n_decoder_hidden_3], [n_decoder_hidden_3], phase_train)\n",
    "\n",
    "        with tf.variable_scope(\"output\"):\n",
    "            output = layer(hidden_3, [n_decoder_hidden_3, 784], [784], phase_train)\n",
    "\n",
    "    return output\n",
    "\n",
    "def loss(output, x):\n",
    "    with tf.variable_scope(\"training\"):\n",
    "        l2 = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(output, x)), 1))\n",
    "        train_loss = tf.reduce_mean(l2)\n",
    "        train_summary_op = tf.summary.scalar(\"train_cost\", train_loss)\n",
    "        return train_loss, train_summary_op\n",
    "\n",
    "def training(cost, global_step):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, \n",
    "        use_locking=False, name='Adam')\n",
    "    train_op = optimizer.minimize(cost, global_step=global_step)\n",
    "    return train_op\n",
    "\n",
    "def image_summary(label, tensor):\n",
    "    tensor_reshaped = tf.reshape(tensor, [-1, 28, 28, 1])\n",
    "    return tf.summary.image(label, tensor_reshaped)\n",
    "\n",
    "def evaluate(output, x):\n",
    "    with tf.variable_scope(\"validation\"):\n",
    "        in_im_op = image_summary(\"input_image\", x)\n",
    "        out_im_op = image_summary(\"output_image\", output)\n",
    "        l2 = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(output, x, name=\"val_diff\")), 1))\n",
    "        val_loss = tf.reduce_mean(l2)\n",
    "        val_summary_op = tf.summary.scalar(\"val_cost\", val_loss)\n",
    "        return val_loss, in_im_op, out_im_op, val_summary_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The output of the decoder network is a 784-dimensional vector  that can be reconstructed into a 28 × 28 image.\n",
    "- In order to accelerate training, we’ll reuse the batch normalization strategy we employed in Chapter 5.\n",
    "- Loss\n",
    "    - Given an input vector upper I and a reconstruction upper O\n",
    "    - $\\lVert I-O \\rVert=\\sqrt{\\sum_i(I_i-O_i)^2}$, also known as the L2 norm of the difference between the two vectors\n",
    "- Evaluate\n",
    "    - Use a validation dataset and compute the same L2 norm measurement for model evaluation.\n",
    "    - Collect image summaries so that we can compare both the input images and the reconstructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost = 5.726947857\n",
      "Validation Loss: 4.45524\n",
      "Epoch: 0101 cost = 1.875294229\n",
      "Validation Loss: 1.81979\n",
      "Epoch: 0201 cost = 1.757120199\n",
      "Validation Loss: 1.76411\n",
      "Epoch: 0301 cost = 1.665520479\n",
      "Validation Loss: 1.77019\n",
      "Epoch: 0401 cost = 1.611918144\n",
      "Validation Loss: 1.75243\n",
      "Optimization Finished!\n",
      "Test Loss: <function loss at 0x10462baa0>\n"
     ]
    }
   ],
   "source": [
    "import input_data\n",
    "mnist = input_data.read_data_sets(\"data/\", one_hot=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Architecture\n",
    "n_encoder_hidden_1 = 1000\n",
    "n_encoder_hidden_2 = 500\n",
    "n_encoder_hidden_3 = 250\n",
    "n_code = 30\n",
    "n_decoder_hidden_1 = 250\n",
    "n_decoder_hidden_2 = 500\n",
    "n_decoder_hidden_3 = 1000\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 500\n",
    "batch_size = 100\n",
    "display_step = 100\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    with tf.variable_scope(\"autoencoder_model\"):\n",
    "        x = tf.placeholder(\"float\", [None, 784]) # mnist data image of shape 28*28=784\n",
    "        phase_train = tf.placeholder(tf.bool)\n",
    "\n",
    "        code = encoder(x, phase_train)\n",
    "        output = decoder(code, phase_train)\n",
    "        cost, train_summary_op = loss(output, x)\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        train_op = training(cost, global_step)\n",
    "        eval_op, in_im_op, out_im_op, val_summary_op = evaluate(output, x)\n",
    "        summary_op = tf.summary.merge_all()\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        sess = tf.Session()\n",
    "        train_writer = tf.summary.FileWriter(\"mnist_autoencoder_logs/\", graph=sess.graph)\n",
    "        val_writer = tf.summary.FileWriter(\"mnist_autoencoder_logs/\",graph=sess.graph)\n",
    "\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess.run(init_op)\n",
    "\n",
    "        # Training cycle\n",
    "        for epoch in range(training_epochs):\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(mnist.train.num_examples/batch_size)\n",
    "            # Loop over all batches\n",
    "            for i in range(total_batch):\n",
    "                minibatch_x, minibatch_y = mnist.train.next_batch(batch_size)\n",
    "                # Fit training using batch data\n",
    "                _, new_cost, train_summary = sess.run([train_op, cost, train_summary_op], feed_dict={x: minibatch_x, phase_train: True})\n",
    "                train_writer.add_summary(train_summary, sess.run(global_step))\n",
    "                # Compute average loss\n",
    "                avg_cost += new_cost/total_batch\n",
    "            # Display logs per epoch step\n",
    "            if epoch % display_step == 0:\n",
    "                print \"Epoch:\", '%04d' % (epoch+1), \"cost =\", \"{:.9f}\".format(avg_cost)\n",
    "\n",
    "                validation_loss, in_im, out_im, val_summary = sess.run([eval_op, in_im_op, out_im_op, val_summary_op], feed_dict={x: mnist.validation.images, phase_train: False})\n",
    "                val_writer.add_summary(in_im, sess.run(global_step))\n",
    "                val_writer.add_summary(out_im, sess.run(global_step))\n",
    "                val_writer.add_summary(val_summary, sess.run(global_step))\n",
    "                print \"Validation Loss:\", validation_loss\n",
    "                saver.save(sess, \"mnist_autoencoder_logs/model-checkpoint\", global_step=global_step)\n",
    "\n",
    "        print \"Optimization Finished!\"\n",
    "        test_loss = sess.run(eval_op, feed_dict={x: mnist.test.images, phase_train: False})\n",
    "        print \"Test Loss:\", loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The reconstructions for three randomly chosen samples from the test set are shown in Figure 6-9.\n",
    "\n",
    "![6-9](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0609.png)\n",
    "\n",
    "Figure 6-9. A side-by-side comparison of the original inputs (from the validation set) and reconstructions after 5, 100, and 200 epochs of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from sklearn import decomposition\n",
    "import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"data/\", one_hot=False)\n",
    "pca = decomposition.PCA(n_components=2)\n",
    "pca.fit(mnist.train.images)\n",
    "pca_codes = pca.transform(mnist.test.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `one_hot=False`:labels to be provided as integers instead of one-hot vectors.\n",
    "- `n_components=2`:to generate two-dimensional codes.\n",
    "\n",
    "We can also reconstruct the original images from the two-dimensional codes and visualize the reconstructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1046f5710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "pca_recon = pca.inverse_transform(pca_codes[:1])\n",
    "plt.imshow(pca_recon[0].reshape((28,28)), cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![6-10](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0610.png)\n",
    "\n",
    "Figure 6-10. Comparing the reconstructions by both PCA and autoencoder side by side\n",
    "\n",
    "In the resulting visualization in Figure 6-11, it is extremely difficult to make out separable clusters in the two-dimensional PCA codes; the autoencoder has clearly done a spectacular job at clustering codes of different digit classes. This means that **a simple machine learning model is going to be able to much more effectively classify data points consisting of autoencoder embeddings as compared to PCA embeddings.**\n",
    "\n",
    "![6-11](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0611.png)\n",
    "\n",
    "Figure 6-11. We visualize two-dimensional embeddings produced by PCA (left) and by an autoencoder (right). Notice that the autoencoder does a much better job of clustering codes of different digit classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoising to Force Robust Representations\n",
    "![6-12](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0612.png)\n",
    "\n",
    "Figure 6-12. In the top row, we have original images from the MNIST dataset. In the bottom row, we’ve randomly blacked out half of the pixels. Despite the corruption, the digits in the bottom row are still identifiable by human perception. \n",
    "\n",
    "This is a property we might hope to enforce in our embedding algorithm, and it was first explored by Vincent et al. in 2008, when they introduced the `denoising autoencoder`.\n",
    "\n",
    "Let’s say we had a two-dimensional dataset with various labels. Let’s take all of the data points in a particular category (i.e., with some fixed label), and call this subset of data points S. While any arbitrary sampling of points could end up taking any form while visualized, we presume that for real-life categories, there is some underlying structure that unifies all of the points in S. This underlying, unifying geometric structure is known as a `manifold`. The manifold is the shape that we want to capture when we reduce the dimensionality of our data; and as Alain and Bengio described in 2014, our autoencoder is implicitly learning this manifold as it learns how to reconstruct data after pushing it through a bottleneck (the code layer). The autoencoder must figure out whether a point belongs to one manifold or another when trying to generate a reconstruction of an instance with potentially different labels.\n",
    "\n",
    "![6-14](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0614.png)\n",
    "\n",
    "Figure 6-14. We apply a corruption operation to the dataset and train a denoising autoencoder to reconstruct the original, uncorrupted images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparsity in Autoencoders\n",
    "`Interpretability` is a property of a machine learning model that measures how easy it is to inspect and explain its process and/or output. Deep models are generally very difficult to interpret because of the nonlinearities and massive numbers of parameters that make up a model.\n",
    "\n",
    "In general, an autoencoder’s representations are dense, and this has implications with respect to how the representation changes as we make coherent modifications to the input. Consider the situation in Figure 6-15.\n",
    "\n",
    "![6-15](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0615.png)\n",
    "\n",
    "Figure 6-15. The activations of a dense representation combine and overlay information from multiple features in ways that are difficult to interpret\n",
    "\n",
    "The ideal outcome for us is if we can build a representation where there is a 1-to-1 correspondence, or close to a 1-to-1 correspondence, between high-level features and individual components in the code. When we are able to achieve this, we get very close to the system described in Figure 6-16. Part A of the figure shows how the representation changes as we add and remove components, and part B color-codes the correspondence between strokes and the components in the code. In this setup, it’s quite clear how and why the representation changes—the representation is very clearly the sum of the individual strokes in the image.\n",
    "\n",
    "![6-16](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0616.png)\n",
    "\n",
    "Figure 6-16. With the right combination of space and sparsity, a representation is more interpretable. In A, we show how activations in the representation change with the addition and removal of strokes. In B, we color-code the activations that correspond to each stroke to highlight our ability to interpret how a stroke affects the representation. \n",
    "\n",
    "We’ve explored how we can use autoencoders to find strong representations of data points by summarizing their content. This mechanism of dimensionality reduction works well when the independent data points are rich and contain all of the relevant information pertaining to their structure in their original representation.\n",
    "\n",
    "## When Context Is More Informative than the Input Vector\n",
    "![6-17](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0617.png)\n",
    "\n",
    "Figure 6-17. An example of generating one-hot vector representations for words using a simple document\n",
    "\n",
    "Overall, as illustrated in Figure 6-18,  by analyzing the context (i.e., a fixed window of words surrounding a target word), we can quickly surmise the meaning of the word.\n",
    "\n",
    "![6-18](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0618.png)\n",
    "\n",
    "Figure 6-18. We can identify words with similar meanings based on their contexts. For example, the words “jumps” and “leaps” should have similar vector representations because they are virtually interchangeable. Moreover, we can draw conclusions about what the words “jumps” and “leaps” mean just by looking at the words around them. \n",
    "\n",
    "It turns out we can use the same principles we used when building the autoencoder to build a network that builds strong, distributed representations. Two strategies are shown in Figure 6-19.\n",
    "\n",
    "![6-19](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0619.png)\n",
    "\n",
    "Figure 6-19. General architectures for designing encoders and decoders that generate embeddings by mapping words to their respective contexts (A) or vice versa (B)\n",
    "\n",
    "## The Word2Vec Framework\n",
    "The `CBOW`(Continuous Bag of Words) model used the encoder to create an embedding from the full context (treated as one input) and predict the target word.\n",
    "\n",
    "The `Skip-Gram` model does the inverse of CBOW, taking the target word as an input, and then attempting to predict one of the words in the context.\n",
    "\n",
    "Consider the sentence “the boy went to the bank.” If we broke this sentence down into a sequence of (context, target) pairs, we would obtain [([the, went], boy), ([boy, to], went), ([went, the], to), ([to, bank], the)]. Taking this a step further, we have to split each (context, target) pair into (input, output) pairs where the input is the target and the output is one of the words from the context. From the first pair ([the, went], boy), we would generate the two pairs (boy, the) and (boy, went). We continue to apply this operation to every (context, target) pair to build our dataset. Finally, we replace each word with its unique index $i \\in \\{0,1,\\cdots,|V|-1\\}$ corresponding to its index in the vocabulary.\n",
    "\n",
    "The structure of the encoder is surprisingly simple. It is essentially a lookup table with |V| rows, where the $i^{th}$ row is the embedding corresponding to the $i^{th}$ vocabulary word. All the encoder has to do is take the index of the input word and output the appropriate row in the lookup table.\n",
    "\n",
    "We can implement this simply in TensorFlow with the following TensorFlow function:\n",
    "\n",
    "```py\n",
    "tf.nn.embedding_lookup(params, ids, partition_strategy='mod', \n",
    "                       name=None, validate_indices=True)\n",
    "```\n",
    "\n",
    "- `params` is the embedding matrix\n",
    "- `ids` is a tensor of indices we want to look up\n",
    "\n",
    "The naive way to construct the decoder would be to attempt to reconstruct the one-hot encoding vector for the output, which we could implement with a run-of-the-mill feed-forward layer coupled with a softmax. The only concern is that it’s inefficient because we have to produce a probability distribution over the whole vocabulary space.\n",
    "\n",
    "To reduce the number of parameters, Mikolov et al. used a strategy for implementing the decoder known as `noise-contrastive estimation` (NCE). The strategy is illustrated in Figure 6-20.\n",
    "\n",
    "![6-20](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0620.png)\n",
    "\n",
    "Figure 6-20. An illustration of how noise-contrastive estimation works. A binary logistic regression compares the embedding of the target with the embedding of a context word and randomly sampled noncontext words. We construct a loss function describing how effectively the embeddings enable identification of words in the context of the target versus words outside the context of the target. \n",
    "\n",
    "The NCE strategy uses the lookup table to find the embedding for the output, as well as embeddings for random selections from the vocabulary that are not in the context of the input. We then employ a binary logistic regression model that, one at a time, takes the input embedding and the embedding of the output or random selection, and then outputs a value between 0 to 1 corresponding to the probability that the comparison embedding represents a vocabulary word present in the input’s context. We then take the sum of the probabilities corresponding to the noncontext comparisons and subtract the probability corresponding to the context comparison. This value is the objective function that we want to minimize (in the optimal scenario where the model has perfect performance, the value will be -1).  Implementing NCE in TensorFlow utilizes the following code snippet:\n",
    "\n",
    "```py\n",
    "tf.nn.nce_loss(weights, biases, inputs, labels, num_sampled, \n",
    "               num_classes, num_true=1, sampled_values=None, \n",
    "               remove_accidental_hits=False, \n",
    "               partition_strategy='mod', \n",
    "               name='nce_loss')\n",
    "```\n",
    "\n",
    "- `weights` should have the same dimensions as the embedding matrix\n",
    "- `biases` should be a tensor with size equal to the vocabulary\n",
    "- `inputs` are the results from the embedding lookup\n",
    "- `num_sampled` is the number of negative samples we use to compute the NCE\n",
    "- `num_classes` is the vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embedding_layer(x, embedding_shape):\n",
    "    with tf.variable_scope(\"embedding\"):\n",
    "        embedding_init = tf.random_uniform(embedding_shape, -1.0, 1.0)\n",
    "        embedding_matrix = tf.get_variable(\"E\", initializer=embedding_init)\n",
    "        return tf.nn.embedding_lookup(embedding_matrix, x), embedding_matrix\n",
    "\n",
    "def noise_contrastive_loss(embedding_lookup, weight_shape, bias_shape, y):\n",
    "    with tf.variable_scope(\"nce\"):\n",
    "        nce_weight_init = tf.truncated_normal(weight_shape, stddev=1.0/(weight_shape[1])**0.5)\n",
    "        nce_bias_init = tf.zeros(bias_shape)\n",
    "        nce_W = tf.get_variable(\"W\", initializer=nce_weight_init)\n",
    "        nce_b = tf.get_variable(\"b\", initializer=nce_bias_init)\n",
    "\n",
    "        total_loss = tf.nn.nce_loss(nce_W, nce_b, y, embedding_lookup, neg_size, data.vocabulary_size)\n",
    "        return tf.reduce_mean(total_loss)\n",
    "\n",
    "def training(cost, global_step):\n",
    "    with tf.variable_scope(\"training\"):\n",
    "        summary_op = tf.summary.scalar(\"cost\", cost)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        train_op = optimizer.minimize(cost, global_step=global_step)\n",
    "        return train_op, summary_op\n",
    "\n",
    "def validation(embedding_matrix, x_val):\n",
    "    norm = tf.reduce_sum(embedding_matrix**2, 1, keep_dims=True)**0.5\n",
    "    normalized = embedding_matrix/norm\n",
    "    val_embeddings = tf.nn.embedding_lookup(normalized, x_val)\n",
    "    cosine_similarity = tf.matmul(val_embeddings, normalized, transpose_b=True)\n",
    "    return normalized, cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `embedding_layer`:initialize the lookup table  with a matrix of values\n",
    "- `noise_contrastive_loss`:compute the NCE cost for each training example, and then compile all of the results in the minibatch into a single measurement\n",
    "- `training`:employ stochastic gradient descent with a learning rate of 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```py\n",
    "import input_word_data as data\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TRAINING PARAMETERS\n",
    "batch_size = 32                                         # Number of training examples per batch\n",
    "embedding_size = 128                                    # Dimension of embedding vectors\n",
    "skip_window = 5                                         # Window size for context to the left and right of target\n",
    "num_skips = 4                                           # How many times to reuse target to generate a label for context.\n",
    "batches_per_epoch = data.data_size*num_skips/batch_size # Number of batches per epoch of training\n",
    "training_epochs = 5                                     # Number of epochs to utilize for training\n",
    "neg_size = 64                                           # Number of negative samples to use for NCE\n",
    "display_step = 20000                                    # Frequency with which to print statistics\n",
    "val_step = 10000                                        # Frequency with which to perform validation\n",
    "learning_rate = 0.1                                     # Learning rate for SGD\n",
    "\n",
    "print \"Epochs: %d, Batches per epoch: %d, Examples per batch: %d\" % (training_epochs, batches_per_epoch, batch_size)\n",
    "\n",
    "# NEAREST NEIGHBORS VALIDATION PARAMETERS\n",
    "val_size = 20\n",
    "val_dist_span = 500\n",
    "val_examples = np.random.choice(val_dist_span, val_size, replace=False)\n",
    "top_match = 8\n",
    "plot_num = 500\n",
    "\n",
    "# Main function\n",
    "with tf.Graph().as_default():\n",
    "    with tf.variable_scope(\"skipgram_model\"):\n",
    "        x = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "        y = tf.placeholder(tf.int32, [batch_size, 1])\n",
    "        val = tf.constant(val_examples, dtype=tf.int32)\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "        e_lookup, e_matrix = embedding_layer(x, [data.vocabulary_size, embedding_size])\n",
    "\n",
    "        cost = noise_contrastive_loss(e_lookup, [data.vocabulary_size, embedding_size], [data.vocabulary_size], y)\n",
    "\n",
    "        train_op, summary_op = training(cost, global_step)\n",
    "\n",
    "        val_op = validation(e_matrix, val)\n",
    "\n",
    "        sess = tf.Session()\n",
    "\n",
    "        train_writer = tf.summary.FileWriter(\"skipgram_logs/\", graph=sess.graph)\n",
    "\n",
    "        init_op = tf.global_variables_initializer()\n",
    "\n",
    "        sess.run(init_op)\n",
    "\n",
    "        step = 0\n",
    "        avg_cost = 0\n",
    "\n",
    "        for epoch in xrange(training_epochs):\n",
    "            for minibatch in xrange(batches_per_epoch):\n",
    "                step += 1\n",
    "                minibatch_x, minibatch_y = data.generate_batch(batch_size, num_skips, skip_window)\n",
    "                feed_dict = {x : minibatch_x, y : minibatch_y}\n",
    "\n",
    "                _, new_cost, train_summary = sess.run([train_op, cost, summary_op], feed_dict=feed_dict)\n",
    "                train_writer.add_summary(train_summary, sess.run(global_step))\n",
    "                # Compute average loss\n",
    "                avg_cost += new_cost/display_step\n",
    "\n",
    "                if step % display_step == 0:\n",
    "                    print \"Elapsed:\", str(step), \"batches. Cost =\", \"{:.9f}\".format(avg_cost)\n",
    "                    avg_cost = 0\n",
    "\n",
    "                if step % val_step == 0:\n",
    "                    _, similarity = sess.run(val_op)\n",
    "                    for i in xrange(val_size):\n",
    "                        val_word = data.reverse_dictionary[val_examples[i]]\n",
    "                        neighbors = (-similarity[i, :]).argsort()[1:top_match+1]\n",
    "                        print_str = \"Nearest neighbor of %s:\" % val_word\n",
    "                        for k in xrange(top_match):\n",
    "                            print_str += \" %s,\" % data.reverse_dictionary[neighbors[k]]\n",
    "                        print print_str[:-1]\n",
    "\n",
    "        final_embeddings, _ = sess.run(val_op)\n",
    "\n",
    "tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "plot_embeddings = np.asfarray(final_embeddings[:plot_num,:], dtype='float')\n",
    "low_dim_embs = tsne.fit_transform(plot_embeddings)\n",
    "labels = [reverse_dictionary[i] for i in xrange(plot_embeddings)]\n",
    "data.plot_with_labels(low_dim_embs, labels)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

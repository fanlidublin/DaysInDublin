{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals of Deep Learning\n",
    "## 目录\n",
    "- Chapter 3. Implementing Neural Networks in TensorFlow\n",
    "    - Installing TensorFlow\n",
    "    - Creating and Manipulating TensorFlow Variables\n",
    "    - TensorFlow Operations\n",
    "    - Placeholder Tensors\n",
    "    - Sessions in TensorFlow\n",
    "    - Navigating Variable Scopes and Sharing Variables\n",
    "    - Managing Models over the CPU and GPU\n",
    "    - Specifying the Logistic Regression Model in TensorFlow\n",
    "    - Logging and Training the Logistic Regression Model\n",
    "    - Leveraging TensorBoard to Visualize Computation Graphs and Learning\n",
    "    - Building a Multilayer Model for MNIST in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3. Implementing Neural Networks in TensorFlow\n",
    "## Installing TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Deep Learning'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "deep_learning = tf.constant('Deep Learning')\n",
    "session = tf.Session()\n",
    "session.run(deep_learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant(2)\n",
    "b = tf.constant(3)\n",
    "multiply = tf.multiply(a, b)\n",
    "session.run(multiply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the [tensorflow 1.0.0 release notes](https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md)\n",
    "\n",
    "> tf.mul, tf.sub and tf.neg are deprecated in favor of tf.multiply, tf.subtract and tf.negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Manipulating TensorFlow Variables\n",
    "```py\n",
    "# Common tensors from the TensorFlow API docs\n",
    "\n",
    "tf.zeros(shape, dtype=tf.float32, name=None)\n",
    "tf.ones(shape, dtype=tf.float32, name=None)\n",
    "tf.random_normal(shape, mean=0.0, stddev=1.0, \n",
    "                 dtype=tf.float32, seed=None, \n",
    "                 name=None)\n",
    "tf.truncated_normal(shape, mean=0.0, stddev=1.0, \n",
    "                    dtype=tf.float32, seed=None, \n",
    "                    name=None)\n",
    "tf.random_uniform(shape, minval=0, maxval=None, \n",
    "                  dtype=tf.float32, seed=None, \n",
    "                  name=None)\n",
    "```\n",
    "\n",
    "When we call `tf.Variable`, three operations are added to the computation graph:\n",
    "\n",
    "1. The operation producing the tensor we use to initialize our variable\n",
    "1. The `tf.assign` operation, which is responsible for filling the variable with the initializing tensor prior to the variable’s use\n",
    "1. The variable operation, which holds the current value of the variable\n",
    "\n",
    "![3-1](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0301.png)\n",
    "\n",
    "Figure 3-1. Three operations are added to the graph when instantiating a TensorFlow variable. In this example, we instantiate the variable weights using a random normal initializer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Operations\n",
    "Table 3-1. A summary table of TensorFlow operations\n",
    "\n",
    "Category | Examples\n",
    "---------|---------\n",
    "Element-wise mathematical operations | Add, Sub, Mul, Div, Exp, Log, Greater, Less, Equal, ...\n",
    "Array operations | Concat, Slice, Split, Constant, Rank, Shape, Shuffle, ...\n",
    "Matrix operations | MatMul, MatrixInverse, MatrixDeterminant, ...\n",
    "Stateful operations | Variable, Assign, AssignAdd, ...\n",
    "Neural network building blocks | SoftMax, Sigmoid, ReLU, Convolution2D, MaxPool, ...\n",
    "Checkpointing operations | Save, Restore\n",
    "Queue and synchronization operations | Enqueue, Dequeue, MutexAcquire, MutexRelease, ...\n",
    "Control flow operations | Merge, Switch, Enter, Leave, NextIteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholder Tensors\n",
    "A `variable` is insufficient because it is only meant to be initialized once. Just as variables need to be initialized the first time the computation graph is built, `placeholders` need to be filled every time the computation graph (or a subgraph) is run. \n",
    "\n",
    "```py\n",
    "x = tf.placeholder(tf.float32, name=\"x\", shape=[None, 784])\n",
    "W = tf.Variable(tf.random_uniform([784,10], -1, 1), name=\"W\")\n",
    "multiply = tf.matmul(x, W)\n",
    "```\n",
    "\n",
    "- define a placeholder where x represents a minibatch of data stored as float32’s\n",
    "- x has 784 columns\n",
    "- x has an undefined number of rows. This means that x can be initialized with an arbitrary number of data samples\n",
    "- W demension:784*10, range $\\in$ [-1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sessions in TensorFlow\n",
    "```py\n",
    "import tensorflow as tf\n",
    "from read_data import get_minibatch()\n",
    "\n",
    "x = tf.placeholder(tf.float32, name=\"x\", shape=[None, 784])\n",
    "W = tf.Variable(tf.random_uniform([784, 10], -1, 1), name=\"W\")\n",
    "b = tf.Variable(tf.zeros([10]), name=\"biases\")\n",
    "output = tf.matmul(x, W) + b\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session() \n",
    "sess.run(init_op)\n",
    "feed_dict = {\"x\" : get_minibatch()}\n",
    "sess.run(output, feed_dict=feed_dict)\n",
    "```\n",
    "\n",
    "![3-2](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0302.png)\n",
    "\n",
    "Figure 3-2. This is a an example of a simple computational graph in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigating Variable Scopes and Sharing Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_network(input):\n",
    "    W_1 = tf.Variable(tf.random_uniform([784, 100], -1, 1), name=\"W_1\")\n",
    "    b_1 = tf.Variable(tf.zeros([100]), name=\"biases_1\")\n",
    "    output_1 = tf.matmul(input, W_1) + b_1\n",
    "\n",
    "    W_2 = tf.Variable(tf.random_uniform([100, 50], -1, 1), name=\"W_2\")\n",
    "    b_2 = tf.Variable(tf.zeros([50]), name=\"biases_2\")\n",
    "    output_2 = tf.matmul(output_1, W_2) + b_2\n",
    "\n",
    "    W_3 = tf.Variable(tf.random_uniform([50, 10], -1, 1), name=\"W_3\")\n",
    "    b_3 = tf.Variable(tf.zeros([10]), name=\"biases_3\")\n",
    "    output_3 = tf.matmul(output_2, W_3) + b_3\n",
    "\n",
    "    # printing names\n",
    "    print \"Printing names of weight parameters\"\n",
    "    print W_1.name, W_2.name, W_3.name\n",
    "    print \"Printing names of bias parameters\"\n",
    "    print b_1.name, b_2.name, b_3.name\n",
    "\n",
    "    return output_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing names of weight parameters\n",
      "W_1:0 W_2:0 W_3:0\n",
      "Printing names of bias parameters\n",
      "biases_1:0 biases_2:0 biases_3:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_2:0' shape=(1000, 10) dtype=float32>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_1 = tf.placeholder(tf.float32, [1000, 784], name=\"i_1\")\n",
    "my_network(i_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing names of weight parameters\n",
      "W_1_1:0 W_2_1:0 W_3_1:0\n",
      "Printing names of bias parameters\n",
      "biases_1_1:0 biases_2_1:0 biases_3_1:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_5:0' shape=(1000, 10) dtype=float32>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_2 = tf.placeholder(tf.float32, [1000, 784], name=\"i_2\")\n",
    "my_network(i_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow’s variable scoping mechanisms are largely controlled by two functions:\n",
    "\n",
    "- `tf.get_variable(<name>, <shape>, <initializer>)`\n",
    "    - Checks if a variable with this name exists, retrieves the variable if it does, or creates it using the shape and initializer if it doesn’t.\n",
    "- `tf.variable_scope(<scope_name>)`\n",
    "    - Manages the namespace and determines the scope in which `tf.get_variable` operates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def layer(input, weight_shape, bias_shape):\n",
    "    weight_init = tf.random_uniform_initializer(minval=-1, maxval=1)\n",
    "    bias_init = tf.constant_initializer(value=0)\n",
    "    W = tf.get_variable(\"W\", weight_shape, initializer=weight_init)\n",
    "    b = tf.get_variable(\"b\", bias_shape, initializer=bias_init)\n",
    "    return tf.matmul(input, W) + b\n",
    "\n",
    "def my_network(input):\n",
    "    with tf.variable_scope(\"layer_1\"):\n",
    "        output_1 = layer(input, [784, 100], [100])\n",
    "     \n",
    "    with tf.variable_scope(\"layer_2\"):\n",
    "        output_2 = layer(output_1, [100, 50], [50])\n",
    "     \n",
    "    with tf.variable_scope(\"layer_3\"):\n",
    "        output_3 = layer(output_2, [50, 10], [10])\n",
    "     \n",
    "    return output_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s try to call `my_network` twice, just like we did in the preceding code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'layer_3/add:0' shape=(1000, 10) dtype=float32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_1 = tf.placeholder(tf.float32, [1000, 784], name=\"i_1\")\n",
    "my_network(i_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable layer_1/W already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"<ipython-input-6-fd851d0f603b>\", line 4, in layer\n    W = tf.get_variable(\"W\", weight_shape, initializer=weight_init)\n  File \"<ipython-input-6-fd851d0f603b>\", line 10, in my_network\n    output_1 = layer(input, [784, 100], [100])\n  File \"<ipython-input-7-0155c84ecd17>\", line 2, in <module>\n    my_network(i_1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e22e9d54b2f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mi_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m784\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"i_2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmy_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-fd851d0f603b>\u001b[0m in \u001b[0;36mmy_network\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmy_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m      \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"layer_1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m           \u001b[0moutput_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m      \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"layer_2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-fd851d0f603b>\u001b[0m in \u001b[0;36mlayer\u001b[0;34m(input, weight_shape, bias_shape)\u001b[0m\n\u001b[1;32m      2\u001b[0m      \u001b[0mweight_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_uniform_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m      \u001b[0mbias_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m      \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"W\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m      \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"b\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m      \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zhangjun/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m   1047\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m       use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m   1050\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1051\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m/Users/zhangjun/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m    946\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m           use_resource=use_resource, custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/Users/zhangjun/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\u001b[0m\n\u001b[1;32m    354\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m           validate_shape=validate_shape, use_resource=use_resource)\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m/Users/zhangjun/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource)\u001b[0m\n\u001b[1;32m    339\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m           use_resource=use_resource)\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zhangjun/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource)\u001b[0m\n\u001b[1;32m    651\u001b[0m                          \u001b[0;34m\" Did you mean to set reuse=True in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 653\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    654\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable layer_1/W already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"<ipython-input-6-fd851d0f603b>\", line 4, in layer\n    W = tf.get_variable(\"W\", weight_shape, initializer=weight_init)\n  File \"<ipython-input-6-fd851d0f603b>\", line 10, in my_network\n    output_1 = layer(input, [784, 100], [100])\n  File \"<ipython-input-7-0155c84ecd17>\", line 2, in <module>\n    my_network(i_1)\n"
     ]
    }
   ],
   "source": [
    "i_2 = tf.placeholder(tf.float32, [1000, 784], name=\"i_2\")\n",
    "my_network(i_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, sharing is not allowed (just to be safe!), but if we want to enable sharing within a variable scope, we can say so explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"shared_variables\") as scope:\n",
    "    i_1 = tf.placeholder(tf.float32, [1000, 784], name=\"i_1\")\n",
    "    my_network(i_1)\n",
    "    scope.reuse_variables()\n",
    "    i_2 = tf.placeholder(tf.float32, [1000, 784], name=\"i_2\")\n",
    "    my_network(i_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Models over the CPU and GPU\n",
    "Supported devices are represented by string IDs and normally consist of the following:\n",
    "\n",
    "- \"/cpu:0\"\n",
    "    - The CPU of our machine.\n",
    "- \"/gpu:0\"\n",
    "    - The first GPU of our machine, if it has one.\n",
    "- \"/gpu:1\"\n",
    "    - The second GPU of our machine, if it has one.\n",
    "\n",
    "To inspect which devices are used by the computational graph, we can initialize our TensorFlow session with the `log_device_placement` set to `True`:\n",
    "\n",
    "```py\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "```\n",
    "\n",
    "If we desire to use a specific device, we may do so by using with `tf.device` to select the appropriate device. If the chosen device is not available, however, an error will be thrown. If we would like TensorFlow to find another available device if the chosen device does not exist, we can pass the `allow_soft_placement` flag to the session variable as follows:\n",
    "\n",
    "```py\n",
    "with tf.device('/gpu:2'):\n",
    "  a = tf.constant([1.0, 2.0, 3.0, 4.0], shape=[2, 2], name='a')\n",
    "  b = tf.constant([1.0, 2.0], shape=[2, 1], name='b')\n",
    "  c = tf.matmul(a, b)\n",
    "\n",
    "sess = tf.Session(config=tf.ConfigProto(\n",
    "      allow_soft_placement=True, log_device_placement=True))\n",
    "\n",
    "sess.run(c)\n",
    "```\n",
    "\n",
    "The following code is an example of multi-GPU code[fig 3-3]:  \n",
    "\n",
    "```py\n",
    "c = []\n",
    "\n",
    "for d in ['/gpu:0', '/gpu:1']:\n",
    "  with tf.device(d):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0], shape=[2, 2], \n",
    "                      name='a')\n",
    "    b = tf.constant([1.0, 2.0], shape=[2, 1], name='b')\n",
    "    c.append(tf.matmul(a, b))\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "  sum = tf.add_n(c)\n",
    "\n",
    "sess = tf.Session(config=tf.ConfigProto(\n",
    "                   log_device_placement=True))\n",
    "\n",
    "sess.run(sum)\n",
    "```\n",
    "\n",
    "![3-3](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0303.png)\n",
    "\n",
    "Figure 3-3. Building multi-GPU models in a tower-like fashion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying the Logistic Regression Model in TensorFlow\n",
    "Our model uses a matrix W representing the weights of the connections in the network, as well as a vector b corresponding to the biases to estimate whether an input x belongs to class i using the softmax expression we talked about earlier:\n",
    "\n",
    "$$p(y=i|x)=softmax_i(Wx+b)=\\frac{e^{W_ix+b_i}}{\\sum_j e^{W_jx+b_j}}$$\n",
    "\n",
    "![3-4](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0304.png)\n",
    "\n",
    "Figure 3-4. Interpreting logistic regression as a primitive neural network\n",
    "\n",
    "We’ll build the the logistic regression model in four phases:\n",
    "\n",
    "1. inference: produces a probability distribution over the output classes given a minibatch\n",
    "1. loss: computes the value of the error function (in this case, the cross-entropy loss)\n",
    "1. training: responsible for computing the gradients of the model’s parameters and updating the model\n",
    "1. evaluate: will determine the effectiveness of a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(x):\n",
    "    init = tf.constant_initializer(value=0)\n",
    "    W = tf.get_variable(\"W\", [784, 10], initializer=init)\n",
    "    b = tf.get_variable(\"b\", [10], initializer=init)\n",
    "    output = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "    w_hist = tf.summary.histogram(\"weights\", W)\n",
    "    b_hist = tf.summary.histogram(\"biases\", b)\n",
    "    y_hist = tf.summary.histogram(\"output\", output)\n",
    "\n",
    "    return output\n",
    "\n",
    "def loss(output, y):\n",
    "    dot_product = y * tf.log(output)\n",
    "\n",
    "    # Reduction along axis 0 collapses each column into a\n",
    "    # single value, whereas reduction along axis 1 collapses  \n",
    "    # each row into a single value. In general, reduction along  \n",
    "    # axis i collapses the ith dimension of a tensor to size 1.\n",
    "    xentropy = -tf.reduce_sum(dot_product, reduction_indices=1)\n",
    "    \n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    return loss\n",
    "\n",
    "def training(cost, global_step):\n",
    "    tf.summary.scalar(\"cost\", cost)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(cost, global_step=global_step)\n",
    "    return train_op\n",
    "\n",
    "def evaluate(output, y):\n",
    "    correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar(\"validation error\", (1.0 - accuracy))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `tf.constant_initializer` 常量初始化函数。\n",
    "- `tf.reduce_sum` 求和，由于求和的对象是tensor，所以是沿着tensor的某些维度求和。\n",
    "    - `reduction_indice` 是指沿tensor的哪些维度求和。\n",
    "    - ![reduction_indice](http://ou8qjsj0m.bkt.clouddn.com//17-8-13/25674240.jpg)\n",
    "- `tf.equal` 是对比这两个矩阵或者向量的相等的元素，如果是相等的那就返回True，反正返回False，返回的值的矩阵维度和A是一样的。\n",
    "    - 下面例子代码输出：`[[ True  True  True False False]]`\n",
    "\n",
    "```py\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "A = [[1,3,4,5,6]]\n",
    "B = [[1,3,4,3,2]]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(tf.equal(A, B)))\n",
    "```\n",
    "\n",
    "- `tf.summary.scalar` and `tf.summary.histogram` commands to log the cost for each minibatch, validation error, and the distribution of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging and Training the Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n",
      "INFO:tensorflow:Summary name validation error is illegal; using validation_error instead.\n",
      "Epoch: 0001 cost = 1.174406662\n",
      "Validation Error: 0.1507999897\n",
      "Epoch: 0002 cost = 0.661975267\n",
      "Validation Error: 0.127799987793\n",
      "Epoch: 0003 cost = 0.550479775\n",
      "Validation Error: 0.119799971581\n",
      "Epoch: 0004 cost = 0.496666517\n",
      "Validation Error: 0.114000022411\n",
      "Epoch: 0005 cost = 0.463729115\n",
      "Validation Error: 0.109600007534\n",
      "Epoch: 0006 cost = 0.440869012\n",
      "Validation Error: 0.106599986553\n",
      "Epoch: 0007 cost = 0.423876265\n",
      "Validation Error: 0.104600012302\n",
      "Epoch: 0008 cost = 0.410587048\n",
      "Validation Error: 0.102800011635\n",
      "Epoch: 0009 cost = 0.399861779\n",
      "Validation Error: 0.100000023842\n",
      "Epoch: 0010 cost = 0.390921709\n",
      "Validation Error: 0.0985999703407\n",
      "Epoch: 0011 cost = 0.383350472\n",
      "Validation Error: 0.0975999832153\n",
      "Epoch: 0012 cost = 0.376742928\n",
      "Validation Error: 0.0957999825478\n",
      "Epoch: 0013 cost = 0.370986774\n",
      "Validation Error: 0.0952000021935\n",
      "Epoch: 0014 cost = 0.365939115\n",
      "Validation Error: 0.093800008297\n",
      "Epoch: 0015 cost = 0.361372554\n",
      "Validation Error: 0.0917999744415\n",
      "Epoch: 0016 cost = 0.357271555\n",
      "Validation Error: 0.090399980545\n",
      "Epoch: 0017 cost = 0.353570737\n",
      "Validation Error: 0.0889999866486\n",
      "Epoch: 0018 cost = 0.350133936\n",
      "Validation Error: 0.0878000259399\n",
      "Epoch: 0019 cost = 0.347030580\n",
      "Validation Error: 0.0888000130653\n",
      "Epoch: 0020 cost = 0.344142686\n",
      "Validation Error: 0.0870000123978\n",
      "Epoch: 0021 cost = 0.341469975\n",
      "Validation Error: 0.087199985981\n",
      "Epoch: 0022 cost = 0.338988112\n",
      "Validation Error: 0.0866000056267\n",
      "Epoch: 0023 cost = 0.336671517\n",
      "Validation Error: 0.0860000252724\n",
      "Epoch: 0024 cost = 0.334482374\n",
      "Validation Error: 0.0852000117302\n",
      "Epoch: 0025 cost = 0.332421122\n",
      "Validation Error: 0.0856000185013\n",
      "Epoch: 0026 cost = 0.330536905\n",
      "Validation Error: 0.0856000185013\n",
      "Epoch: 0027 cost = 0.328713858\n",
      "Validation Error: 0.0849999785423\n",
      "Epoch: 0028 cost = 0.327041571\n",
      "Validation Error: 0.0849999785423\n",
      "Epoch: 0029 cost = 0.325404955\n",
      "Validation Error: 0.0845999717712\n",
      "Epoch: 0030 cost = 0.323815571\n",
      "Validation Error: 0.0838000178337\n",
      "Epoch: 0031 cost = 0.322372914\n",
      "Validation Error: 0.084399998188\n",
      "Epoch: 0032 cost = 0.320976514\n",
      "Validation Error: 0.0827999711037\n",
      "Epoch: 0033 cost = 0.319625769\n",
      "Validation Error: 0.0838000178337\n",
      "Epoch: 0034 cost = 0.318344331\n",
      "Validation Error: 0.0834000110626\n",
      "Epoch: 0035 cost = 0.317125423\n",
      "Validation Error: 0.0834000110626\n",
      "Epoch: 0036 cost = 0.315953904\n",
      "Validation Error: 0.0825999975204\n",
      "Epoch: 0037 cost = 0.314849404\n",
      "Validation Error: 0.0830000042915\n",
      "Epoch: 0038 cost = 0.313760099\n",
      "Validation Error: 0.0827999711037\n",
      "Epoch: 0039 cost = 0.312705152\n",
      "Validation Error: 0.0820000171661\n",
      "Epoch: 0040 cost = 0.311696270\n",
      "Validation Error: 0.0821999907494\n",
      "Epoch: 0041 cost = 0.310747934\n",
      "Validation Error: 0.081200003624\n",
      "Epoch: 0042 cost = 0.309776564\n",
      "Validation Error: 0.0816000103951\n",
      "Epoch: 0043 cost = 0.308896091\n",
      "Validation Error: 0.0802000164986\n",
      "Epoch: 0044 cost = 0.308059065\n",
      "Validation Error: 0.0806000232697\n",
      "Epoch: 0045 cost = 0.307155169\n",
      "Validation Error: 0.0792000293732\n",
      "Epoch: 0046 cost = 0.306371177\n",
      "Validation Error: 0.0795999765396\n",
      "Epoch: 0047 cost = 0.305602618\n",
      "Validation Error: 0.0794000029564\n",
      "Epoch: 0048 cost = 0.304778637\n",
      "Validation Error: 0.0799999833107\n",
      "Epoch: 0049 cost = 0.304078800\n",
      "Validation Error: 0.0788000226021\n",
      "Epoch: 0050 cost = 0.303349025\n",
      "Validation Error: 0.0781999826431\n",
      "Epoch: 0051 cost = 0.302626471\n",
      "Validation Error: 0.0785999894142\n",
      "Epoch: 0052 cost = 0.301978953\n",
      "Validation Error: 0.0781999826431\n",
      "Epoch: 0053 cost = 0.301298777\n",
      "Validation Error: 0.0781999826431\n",
      "Epoch: 0054 cost = 0.300633167\n",
      "Validation Error: 0.077799975872\n",
      "Epoch: 0055 cost = 0.300007916\n",
      "Validation Error: 0.077799975872\n",
      "Epoch: 0056 cost = 0.299402576\n",
      "Validation Error: 0.0774000287056\n",
      "Epoch: 0057 cost = 0.298825037\n",
      "Validation Error: 0.0776000022888\n",
      "Epoch: 0058 cost = 0.298242218\n",
      "Validation Error: 0.0771999955177\n",
      "Epoch: 0059 cost = 0.297690147\n",
      "Validation Error: 0.0771999955177\n",
      "Epoch: 0060 cost = 0.297135488\n",
      "Validation Error: 0.0781999826431\n",
      "Optimization Finished!\n",
      "Test Accuracy: 0.9194\n"
     ]
    }
   ],
   "source": [
    "import input_data\n",
    "mnist = input_data.read_data_sets(\"data/\", one_hot=True)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 60\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    x = tf.placeholder(\"float\", [None, 784]) # mnist data image of shape 28*28=784\n",
    "    y = tf.placeholder(\"float\", [None, 10]) # 0-9 digits recognition => 10 classes\n",
    "\n",
    "    output = inference(x)\n",
    "    cost = loss(output, y)\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    train_op = training(cost, global_step)\n",
    "    eval_op = evaluate(output, y)\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    summary_writer = tf.summary.FileWriter(\"logistic_logs/\", graph=sess.graph)\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            minibatch_x, minibatch_y = mnist.train.next_batch(batch_size)\n",
    "            # Fit training using batch data\n",
    "            sess.run(train_op, feed_dict={x: minibatch_x, y: minibatch_y})\n",
    "            # Compute average loss\n",
    "            avg_cost += sess.run(cost, feed_dict={x: minibatch_x, y: minibatch_y})/total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print \"Epoch:\", '%04d' % (epoch+1), \"cost =\", \"{:.9f}\".format(avg_cost)\n",
    "\n",
    "            accuracy = sess.run(eval_op, feed_dict={x: mnist.validation.images, y: mnist.validation.labels})\n",
    "\n",
    "            print \"Validation Error:\", (1 - accuracy)\n",
    "\n",
    "            summary_str = sess.run(summary_op, feed_dict={x: minibatch_x, y: minibatch_y})\n",
    "            summary_writer.add_summary(summary_str, sess.run(global_step))\n",
    "\n",
    "            saver.save(sess, \"logistic_logs/model-checkpoint\", global_step=global_step)\n",
    "\n",
    "print \"Optimization Finished!\"\n",
    "accuracy = sess.run(eval_op, feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
    "print \"Test Accuracy:\", accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every epoch, we run the `tf.summary.merge_all` in  order to collect all summary statistics we’ve logged and use a `tf.summary.FileWriter` to write the log to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leveraging TensorBoard to Visualize Computation Graphs and Learning\n",
    "TensorFlow comes with a visualization tool called `TensorBoard`, which provides an easy-to-use interface for navigating through our summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting TensorBoard b'39' on port 6006\n",
      "(You can navigate to http://192.168.199.139:6006)\n",
      "WARNING:tensorflow:Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "WARNING:tensorflow:Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "WARNING:tensorflow:Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "^CTraceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.5/bin/tensorboard\", line 11, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/tensorboard/tensorboard.py\", line 151, in main\n",
      "    tb_server.serve_forever()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/socketserver.py\", line 232, in serve_forever\n",
      "    ready = selector.select(poll_interval)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=logistic_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in Figure 3-5, the first tab contains information on the scalar summaries that we collected.\n",
    "\n",
    "![3-5](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0305.png)\n",
    "\n",
    "Figure 3-5. The TensorBoard events view\n",
    "\n",
    "And as Figure 3-6 shows, there’s also a tab that allows us to visualize the full computation graph that we’ve built.\n",
    "\n",
    "![3-6](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0306.png)\n",
    "\n",
    "Figure 3-6. The TensorBoard graph view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Multilayer Model for MNIST in TensorFlow\n",
    "We construct a feed-forward model with two hidden layers, each with 256 ReLU neurons, as shown in Figure 3-7.\n",
    "\n",
    "![3-7](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0307.png)\n",
    "\n",
    "Figure 3-7. A feed-forward network powered by ReLU neurons with two hidden layers\n",
    "\n",
    "The performance of deep neural networks very much depends on an effective initialization of its parameters. For example, changing `tf.random_normal_initializer` back to the `tf.random_uniform_initializer` we used in the logistic regression example significantly hurts performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def layer(input, weight_shape, bias_shape):\n",
    "    weight_init = tf.random_normal_initializer(stddev=(2.0/weight_shape[0])**0.5)\n",
    "    bias_init = tf.constant_initializer(value=0)\n",
    "    W = tf.get_variable(\"W\", weight_shape, initializer=weight_init)\n",
    "    b = tf.get_variable(\"b\", bias_shape, initializer=bias_init)\n",
    "    return tf.nn.relu(tf.matmul(input, W) + b)\n",
    "\n",
    "def inference(x):\n",
    "    with tf.variable_scope(\"hidden_1\"):\n",
    "        hidden_1 = layer(x, [784, n_hidden_1], [n_hidden_1])\n",
    "     \n",
    "    with tf.variable_scope(\"hidden_2\"):\n",
    "        hidden_2 = layer(hidden_1, [n_hidden_1, n_hidden_2], [n_hidden_2])\n",
    "\n",
    "    with tf.variable_scope(\"output\"):\n",
    "        output = layer(hidden_2, [n_hidden_2, 10], [10])\n",
    "\n",
    "    return output\n",
    "\n",
    "def loss(output, y):\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=y)    \n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    return loss\n",
    "\n",
    "def training(cost, global_step):\n",
    "    tf.summary.scalar(\"cost\", cost)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(cost, global_step=global_step)\n",
    "    return train_op\n",
    "\n",
    "def evaluate(output, y):\n",
    "    correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar(\"validation error\", (1.0 - accuracy))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 关于 tf.nn.softmax_cross_entropy_with_logits\n",
    "We perform the softmax while computing the loss instead of during the inference phase of the network.\n",
    "\n",
    "`tf.nn.softmax_cross_entropy_with_logits(logits, labels, name=None)`:\n",
    "- 第一个参数logits：就是神经网络最后一层的输出，如果有batch的话，它的大小就是[batchsize，num_classes]，单样本的话，大小就是num_classes\n",
    "- 第二个参数labels：实际的标签，大小同上\n",
    "\n",
    "具体的执行流程大概分为两步：\n",
    "1. 先对网络最后一层的输出做一个softmax($softmax(x)_i=\\frac{exp(x_i)}{\\sum_j exp(x_j)}$)，这一步通常是求取输出属于某一类的概率，对于单样本而言，输出就是一个num_classes大小的向量（$[Y_1,Y_2,Y_3\\cdots]$其中$Y_1,Y_2,Y_3\\cdots$分别代表了是属于该类的概率）\n",
    "1. softmax的输出向量$[Y_1,Y_2,Y_3\\cdots]$和样本的实际标签做一个交叉熵，公式如下：\n",
    "\n",
    "$$H_{y'}(y)=-\\sum_i y_i' log(y_i)$$\n",
    "\n",
    "- 其中$y_i'$指代实际的标签中第i个的值（用mnist数据举例，如果是3，那么标签是[0，0，0，1，0，0，0，0，0，0]，除了第4个值为1，其他全为0）\n",
    "- $y_i$就是softmax的输出向量$[Y_1,Y_2,Y_3\\cdots]$中，第i个元素的值\n",
    "- 显而易见，预测越准确，结果的值越小（别忘了前面还有负号），最后求一个平均，得到我们想要的loss\n",
    "\n",
    "实例代码：\n",
    "\n",
    "```py\n",
    "import tensorflow as tf\n",
    "\n",
    "#our NN's output\n",
    "logits=tf.constant([[1.0,2.0,3.0],[1.0,2.0,3.0],[1.0,2.0,3.0]])\n",
    "#step1:do softmax\n",
    "y=tf.nn.softmax(logits)\n",
    "#true label\n",
    "y_=tf.constant([[0.0,0.0,1.0],[0.0,0.0,1.0],[0.0,0.0,1.0]])\n",
    "#step2:do cross_entropy\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "#do cross_entropy just one step\n",
    "cross_entropy2=tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_))#dont forget tf.reduce_sum()!!\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    softmax=sess.run(y)\n",
    "    c_e = sess.run(cross_entropy)\n",
    "    c_e2 = sess.run(cross_entropy2)\n",
    "    print(\"step1:softmax result=\")\n",
    "    print(softmax)\n",
    "    print(\"step2:cross_entropy result=\")\n",
    "    print(c_e)\n",
    "    print(\"Function(softmax_cross_entropy_with_logits) result=\")\n",
    "    print(c_e2)\n",
    "```\n",
    "\n",
    "输出结果是：\n",
    "\n",
    "```\n",
    "step1:softmax result=\n",
    "[[ 0.09003057  0.24472848  0.66524094]\n",
    " [ 0.09003057  0.24472848  0.66524094]\n",
    " [ 0.09003057  0.24472848  0.66524094]]\n",
    "step2:cross_entropy result=\n",
    "1.22282\n",
    "Function(softmax_cross_entropy_with_logits) result=\n",
    "1.2228\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n",
      "INFO:tensorflow:Summary name validation error is illegal; using validation_error instead.\n",
      "Epoch: 0001 cost = 1.167901315\n",
      "Validation Error: 0.193799972534\n",
      "Epoch: 0002 cost = 0.583657532\n",
      "Validation Error: 0.0952000021935\n",
      "Epoch: 0003 cost = 0.337281553\n",
      "Validation Error: 0.0824000239372\n",
      "Epoch: 0004 cost = 0.291787940\n",
      "Validation Error: 0.0753999948502\n",
      "Epoch: 0005 cost = 0.264843191\n",
      "Validation Error: 0.0681999921799\n",
      "Epoch: 0006 cost = 0.244155136\n",
      "Validation Error: 0.063000023365\n",
      "Epoch: 0007 cost = 0.227775839\n",
      "Validation Error: 0.0587999820709\n",
      "Epoch: 0008 cost = 0.213482748\n",
      "Validation Error: 0.0572000145912\n",
      "Epoch: 0009 cost = 0.201193266\n",
      "Validation Error: 0.0537999868393\n",
      "Epoch: 0010 cost = 0.190703814\n",
      "Validation Error: 0.0526000261307\n",
      "Epoch: 0011 cost = 0.180840817\n",
      "Validation Error: 0.0483999848366\n",
      "Epoch: 0012 cost = 0.172169948\n",
      "Validation Error: 0.0468000173569\n",
      "Epoch: 0013 cost = 0.164293182\n",
      "Validation Error: 0.0429999828339\n",
      "Epoch: 0014 cost = 0.157237688\n",
      "Validation Error: 0.0422000288963\n",
      "Epoch: 0015 cost = 0.150315270\n",
      "Validation Error: 0.0404000282288\n",
      "Epoch: 0016 cost = 0.143809539\n",
      "Validation Error: 0.0397999882698\n",
      "Epoch: 0017 cost = 0.138288142\n",
      "Validation Error: 0.0383999943733\n",
      "Epoch: 0018 cost = 0.132807644\n",
      "Validation Error: 0.0368000268936\n",
      "Epoch: 0019 cost = 0.127695854\n",
      "Validation Error: 0.0360000133514\n",
      "Epoch: 0020 cost = 0.123071070\n",
      "Validation Error: 0.0360000133514\n",
      "Epoch: 0021 cost = 0.118457685\n",
      "Validation Error: 0.0333999991417\n",
      "Epoch: 0022 cost = 0.114368538\n",
      "Validation Error: 0.035000026226\n",
      "Epoch: 0023 cost = 0.110391890\n",
      "Validation Error: 0.0335999727249\n",
      "Epoch: 0024 cost = 0.106810061\n",
      "Validation Error: 0.0329999923706\n",
      "Epoch: 0025 cost = 0.103084836\n",
      "Validation Error: 0.0314000248909\n",
      "Epoch: 0026 cost = 0.099609090\n",
      "Validation Error: 0.0317999720573\n",
      "Epoch: 0027 cost = 0.096475270\n",
      "Validation Error: 0.0314000248909\n",
      "Epoch: 0028 cost = 0.093608136\n",
      "Validation Error: 0.0310000181198\n",
      "Epoch: 0029 cost = 0.090823234\n",
      "Validation Error: 0.0302000045776\n",
      "Epoch: 0030 cost = 0.087971685\n",
      "Validation Error: 0.0310000181198\n",
      "Epoch: 0031 cost = 0.085327522\n",
      "Validation Error: 0.0284000039101\n",
      "Epoch: 0032 cost = 0.083025288\n",
      "Validation Error: 0.0293999910355\n",
      "Epoch: 0033 cost = 0.080345904\n",
      "Validation Error: 0.0278000235558\n",
      "Epoch: 0034 cost = 0.078043377\n",
      "Validation Error: 0.0275999903679\n",
      "Epoch: 0035 cost = 0.076065099\n",
      "Validation Error: 0.0266000032425\n",
      "Epoch: 0036 cost = 0.073843680\n",
      "Validation Error: 0.0270000100136\n",
      "Epoch: 0037 cost = 0.071988347\n",
      "Validation Error: 0.0271999835968\n",
      "Epoch: 0038 cost = 0.069911680\n",
      "Validation Error: 0.0261999964714\n",
      "Epoch: 0039 cost = 0.068045618\n",
      "Validation Error: 0.0253999829292\n",
      "Epoch: 0040 cost = 0.066214094\n",
      "Validation Error: 0.0260000228882\n",
      "Epoch: 0041 cost = 0.064684716\n",
      "Validation Error: 0.0253999829292\n",
      "Epoch: 0042 cost = 0.062954863\n",
      "Validation Error: 0.0253999829292\n",
      "Epoch: 0043 cost = 0.061380908\n",
      "Validation Error: 0.025200009346\n",
      "Epoch: 0044 cost = 0.059666186\n",
      "Validation Error: 0.0257999897003\n",
      "Epoch: 0045 cost = 0.058348657\n",
      "Validation Error: 0.0248000025749\n",
      "Epoch: 0046 cost = 0.056857154\n",
      "Validation Error: 0.0253999829292\n",
      "Epoch: 0047 cost = 0.055460378\n",
      "Validation Error: 0.0246000289917\n",
      "Epoch: 0048 cost = 0.054099406\n",
      "Validation Error: 0.0246000289917\n",
      "Epoch: 0049 cost = 0.052707463\n",
      "Validation Error: 0.0246000289917\n",
      "Epoch: 0050 cost = 0.051495020\n",
      "Validation Error: 0.0243999958038\n",
      "Epoch: 0051 cost = 0.050231972\n",
      "Validation Error: 0.0243999958038\n",
      "Epoch: 0052 cost = 0.049027650\n",
      "Validation Error: 0.0249999761581\n",
      "Epoch: 0053 cost = 0.048119295\n",
      "Validation Error: 0.0238000154495\n",
      "Epoch: 0054 cost = 0.046903143\n",
      "Validation Error: 0.0238000154495\n",
      "Epoch: 0055 cost = 0.045721100\n",
      "Validation Error: 0.0234000086784\n",
      "Epoch: 0056 cost = 0.044620886\n",
      "Validation Error: 0.0235999822617\n",
      "Epoch: 0057 cost = 0.043739291\n",
      "Validation Error: 0.022400021553\n",
      "Epoch: 0058 cost = 0.042631193\n",
      "Validation Error: 0.022400021553\n",
      "Epoch: 0059 cost = 0.041887093\n",
      "Validation Error: 0.0231999754906\n",
      "Epoch: 0060 cost = 0.040904860\n",
      "Validation Error: 0.022400021553\n",
      "Optimization Finished!\n",
      "Test Accuracy: 0.9744\n"
     ]
    }
   ],
   "source": [
    "import input_data\n",
    "mnist = input_data.read_data_sets(\"data/\", one_hot=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Architecture\n",
    "n_hidden_1 = 256\n",
    "n_hidden_2 = 256\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 60\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    with tf.variable_scope(\"mlp_model\"):\n",
    "        x = tf.placeholder(\"float\", [None, 784]) # mnist data image of shape 28*28=784\n",
    "        y = tf.placeholder(\"float\", [None, 10]) # 0-9 digits recognition => 10 classes\n",
    "\n",
    "        output = inference(x)\n",
    "        cost = loss(output, y)\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        train_op = training(cost, global_step)\n",
    "        eval_op = evaluate(output, y)\n",
    "        summary_op = tf.summary.merge_all()\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        sess = tf.Session()\n",
    "        summary_writer = tf.summary.FileWriter(\"mlp_logs/\", graph=sess.graph)\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess.run(init_op)\n",
    "\n",
    "        # saver.restore(sess, \"mlp_logs/model-checkpoint-66000\")\n",
    "\n",
    "        # Training cycle\n",
    "        for epoch in range(training_epochs):\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(mnist.train.num_examples/batch_size)\n",
    "            # Loop over all batches\n",
    "            for i in range(total_batch):\n",
    "                minibatch_x, minibatch_y = mnist.train.next_batch(batch_size)\n",
    "                # Fit training using batch data\n",
    "                sess.run(train_op, feed_dict={x: minibatch_x, y: minibatch_y})\n",
    "                # Compute average loss\n",
    "                avg_cost += sess.run(cost, feed_dict={x: minibatch_x, y: minibatch_y})/total_batch\n",
    "            # Display logs per epoch step\n",
    "            if epoch % display_step == 0:\n",
    "                print \"Epoch:\", '%04d' % (epoch+1), \"cost =\", \"{:.9f}\".format(avg_cost)\n",
    "                accuracy = sess.run(eval_op, feed_dict={x: mnist.validation.images, y: mnist.validation.labels})\n",
    "                print \"Validation Error:\", (1 - accuracy)\n",
    "\n",
    "                summary_str = sess.run(summary_op, feed_dict={x: minibatch_x, y: minibatch_y})\n",
    "                summary_writer.add_summary(summary_str, sess.run(global_step))\n",
    "\n",
    "                saver.save(sess, \"mlp_logs/model-checkpoint\", global_step=global_step)\n",
    "\n",
    "        print \"Optimization Finished!\"\n",
    "        accuracy = sess.run(eval_op, feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
    "        print \"Test Accuracy:\", accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

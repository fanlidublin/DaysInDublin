{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals of Deep Learning\n",
    "## 目录\n",
    "- Chapter 9. Deep Reinforcement Learning\n",
    "    - Deep Reinforcement Learning Masters Atari Games\n",
    "    - What Is Reinforcement Learning?\n",
    "    - Markov Decision Processes (MDP)\n",
    "        - Policy\n",
    "        - Future Return\n",
    "        - Discounted Future Return\n",
    "    - Policy Versus Value Learning\n",
    "    - Q-Learning and Deep Q-Networks\n",
    "\n",
    "## Deep Reinforcement Learning Masters Atari Games\n",
    "This network, termed a **Deep Q-Network (DQN)** was the first large-scale successful application of reinforcement learning with deep neural networks. DQN was so remarkable because the same architecture, without any changes, was capable of learning 49 different Atari games, despite each game having different rules, goals, and gameplay structure.\n",
    "\n",
    "Later in this chapter we will implement DQN, as it is described in the Nature paper “Human-level control through deep reinforcement learning.”\n",
    "\n",
    "## What Is Reinforcement Learning?\n",
    "This learning process involves an **actor**, an **environment**, and a **reward signal**. The actor chooses to take an action in the environment, for which the actor is rewarded accordingly. The way in which an actor chooses actions is called a **policy**. The actor wants to increase the reward it receives, and so must learn an optimal policy for interacting with the environment (Figure 9-2).\n",
    "\n",
    "![9-2](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0902.png)\n",
    "\n",
    "Figure 9-2. Reinforcement learning setup\n",
    "\n",
    "![9-3](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0903.png)\n",
    "\n",
    "Figure 9-3. A simple reinforcement learning agent balancing a pole. This image is from our OpenAI Gym Policy Gradient agent that we build in this chapter.\n",
    "\n",
    "## Markov Decision Processes (MDP)\n",
    "Our pole-balancing example has a few important elements, which we formalize as a **Markov Decision Process (MDP)**. These elements are:\n",
    "\n",
    "- State\n",
    "    - The cart has a range of possible places on the x-plane where it can be. Similarly, the pole has a range of possible angles.\n",
    "- Action\n",
    "    - The agent can take action by moving the cart either left or right.\n",
    "- State Transition\n",
    "    - When the agent acts, the environment changes—the cart moves and the pole changes angle and velocity.\n",
    "- Reward\n",
    "    - If an agent balances the pole well, it receives a positive reward. If the pole falls, the agent receives a negative reward.\n",
    "\n",
    "An MDP is defined as the following:\n",
    "\n",
    "- S, a finite set of possible states\n",
    "- A, a finite set of actions\n",
    "- P(r,s'|s,a), a state transition function\n",
    "- R, reward function\n",
    "\n",
    "MDPs offer a mathematical framework for modeling decision-making in a given environment (Figure 9-4).\n",
    "\n",
    "![9-4](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0904.png)\n",
    "\n",
    "Figure 9-4. An example of an MDP. Blue circles represent the states of the environment. Red diamonds represent actions that can be taken. The edges from diamonds to circles represent the transition from one state to the next. The numbers along these edges represent the probability of taking a certain action. The numbers at the end of the green arrows represent the reward given to the agent for making the given transition.\n",
    "\n",
    "### Policy\n",
    "MDP’s aim is to find an optimal policy for our agent. Policies are the way in which our agent acts based on its current state.\n",
    "\n",
    "### Future Return\n",
    "Future return is how we consider the rewards of the future. Choosing the best action requires consideration of not only the immediate effects of that action, but also the long-term consequences. Sometimes the best action actually has a negative immediate effect, but a better long-term result. \n",
    "\n",
    "### Discounted Future Return\n",
    "To implement discounted future return, we scale the reward of a current state by the discount factor, , to the power of the current time step. In this way, we penalize agents that take many actions before receiving positive reward. Discounted rewards bias our agent to prefer receiving reward in immediate future, which is advantageous to learning a good policy. \n",
    "\n",
    "## Policy Versus Value Learning\n",
    "In typical supervised learning, we can use stochastic gradient descent to update our parameters to minimize the loss computed from our network’s output and the true label.\n",
    "\n",
    "In reinforcement learning, we don’t have a true label, only reward signals. However, we can still use SGD to optimize our weights using something called **policy gradients**.\n",
    "\n",
    "With our loss function defined, we can apply SGD to minimize our loss and learn a good policy.\n",
    "\n",
    "## Q-Learning and Deep Q-Networks\n",
    "Q-learning is in the category of reinforcement learning called value-learning. Instead of directly learning a policy, we will be learning the value of states and actions. Q-learning involves learning a function, a **Q-function**, which represents the quality of a state, action pair. The Q-function, defined Q(s, a), is a function that calculates the maximum discounted future return when action a is performed in state s.\n",
    "\n",
    "The **Q-value** represents our expected long-term rewards, given we are at a state, and take an action, and then take every subsequent action perfectly (to maximize expected future reward). \n",
    "\n",
    "A question you may be asking is, how can we know Q-values? It is difficult, even for humans, to know how good an action is, because you need to know how you are going to act in the future. Our expected future returns depend on what our long-term strategy is going to be. This seems to be a bit of a chicken-and-egg problem. In order to value a state, action pair you need to know all the perfect subsequent actions. And in order to know the best actions, you need to have accurate values for a state and action.\n",
    "\n",
    "### The Bellman Equation\n",
    "We solve this dilemma by defining our Q-values as a function of future Q-values. This relation is called the Bellman equation, and it states that the maximum future reward for taking action  is the current reward plus the next step’s max future reward from taking the next action a’:\n",
    "\n",
    "$Q^{*}(s_t,a_t)=E[r_t+\\gamma max_a Q^{*}(s_{t+1},a)]$\n",
    "\n",
    "We can use the update rule, then, to propagate that Q-value to the previous time step:\n",
    "\n",
    "$\\hat{Q_j} \\to \\hat{Q_{j+1}} \\to \\hat{Q_{j+2}} \\to \\cdots \\to \\hat{Q}$\n",
    "\n",
    "This updating of the  Q-value is known as **value iteration**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals of Deep Learning\n",
    "## 目录\n",
    "- Chapter 1. The Neural Network\n",
    "    - The Neuron\n",
    "    - Feed-Forward Neural Networks\n",
    "    - Linear Neurons\n",
    "    - Sigmoid, Tanh, and ReLU Neurons\n",
    "- Chapter 2. Training Feed-Forward Neural Networks\n",
    "    - The Fast-Food Problem\n",
    "    - Gradient Descent\n",
    "    - The Delta Rule and Learning Rates\n",
    "    - Gradient Descent with Sigmoidal Neurons\n",
    "    - The Backpropagation Algorithm\n",
    "    - Stochastic and Minibatch Gradient Descent\n",
    "    - Test Sets, Validation Sets, and Overfitting\n",
    "    - Preventing Overfitting in Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Chapter 1. The Neural Network\n",
    "## The Neuron\n",
    "![1-6](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0106.png)\n",
    "\n",
    "Figure 1-6. A functional description of a biological neuron’s structure\n",
    "\n",
    "The neuron receives its inputs along antennae-like structures called `dendrites`. Each of these incoming connections is dynamically strengthened or weakened based on how often it is used (this is how we learn new concepts!), and it’s the strength of each connection that determines the contribution of the input to the neuron’s output. After being weighted by the strength of their respective connections, the inputs are summed together in the `cell body`. This sum is then transformed into a new signal that’s propagated along the cell’s `axon` and sent off to other neurons.\n",
    "\n",
    "![1-7](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0107.png)\n",
    "\n",
    "Figure 1-7. Schematic for a neuron in an artificial neural net\n",
    "\n",
    "Just as in biological neurons, our artificial neuron takes in some number of inputs, $x_1, x_2, \\cdots, x_n$, each of which is multiplied by a specific weight, $w_1, w_2, \\cdots, w_n$. These weighted inputs are, as before, summed together to produce the `logit` of the neuron, $z=\\sum_{i=0}^n w_ix_i$. In many cases, the logit also includes a `bias`, which is a constant (not shown in the figure). The logit is then passed through a function to produce the output . This output can be transmitted to other neurons.\n",
    "\n",
    "Let’s reformulate the inputs as a vector $x = [x_1,x_2,\\cdots,x_n]$ and the weights of the neuron as $w = [w_1,w_2,\\cdots,w_n]$. Then we can re-express the output of the neuron as $y=f(x \\cdot w + b)$, where b is the bias term.\n",
    "\n",
    "## Feed-Forward Neural Networks\n",
    "![1-9](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0109.png)\n",
    "\n",
    "Figure 1-9. A simple example of a feed-forward neural network with three layers (input, one hidden, and output) and three neurons per layer\n",
    "\n",
    "The bottom layer of the network pulls in the input data. The top layer of neurons (output nodes) computes our final answer. The middle layer(s) of neurons are called the hidden layers, and we let $w_{i,j}^{(k)}$ be the weight of the connection between the $i^{th}$ neuron in the $k^{th}$ layer with the $j^{th}$ neuron in the ${k+1}^{st}$ layer. These weights constitute our parameter vector, $\\theta$, our ability to solve problems with neural networks depends on finding the optimal values to plug into $\\theta$.\n",
    "\n",
    "We note that in this example, connections only traverse from a lower layer to a higher layer. There are no connections between neurons in the same layer, and there are no connections that transmit data from a higher layer to a lower layer. These neural networks are called `feed-forward` networks.\n",
    "\n",
    "## Linear Neurons\n",
    "![1-10](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0110.png)\n",
    "\n",
    "Figure 1-10. An example of a linear neuron\n",
    "\n",
    "## Sigmoid, Tanh, and ReLU Neurons\n",
    "### Sigmoid\n",
    "$$f(z)=\\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "![1-11](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0111.png)\n",
    "\n",
    "Figure 1-11. The output of a sigmoid neuron as z varies\n",
    "\n",
    "### Tanh\n",
    "Tanh neurons use a similar kind of S-shaped nonlinearity, but instead of ranging from 0 to 1, the output of tanh neurons range from −1 to 1. \n",
    "\n",
    "![1-12](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0112.png)\n",
    "\n",
    "Figure 1-12. The output of a tanh neuron as z varies\n",
    "\n",
    "### ReLU\n",
    "Restricted linear unit (ReLU) neuron uses the function f(z)=max(0,z), resulting in a characteristic hockey-stick-shaped response, as shown in Figure 1-13.\n",
    "\n",
    "![1-13](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0113.png)\n",
    "\n",
    "Figure 1-13. The output of a ReLU neuron as z varies\n",
    "\n",
    "## Softmax Output Layers\n",
    "We require the sum of all the outputs to be equal to 1. Letting $z_i$ be the logit of the $i^{th}$ softmax neuron, we can achieve this normalization by setting its output to:\n",
    "\n",
    "$$y_i=\\frac{e^{z_i}}{\\sum_i e^{z_j}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2. Training Feed-Forward Neural Networks\n",
    "## The Fast-Food Problem\n",
    "![2-1](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0201.png)\n",
    "\n",
    "Figure 2-1. This is the neuron we want to train for the fast-food problem\n",
    "\n",
    "In this case, let’s say we want to minimize the square error over all of the training examples that we encounter. More formally, if we know that $t^{(i)}$ is the true answer for the $i^{th}$ training example and $y^{(i)}$ is the value computed by the neural network, we want to minimize the value of the error function E:\n",
    "\n",
    "$$E=\\frac{1}{2}\\sum_i(t^{(i)}-y^{(i)})^2$$\n",
    "\n",
    "## Gradient Descent\n",
    "Let’s say our linear neuron only has two inputs(and thus only two weights, $w_1$ and $w_2$). Then we can imagine a three-dimensional space where the horizontal dimensions correspond to the weights $w_1$ and $w_2$, and the vertical dimension corresponds to the value of the error function E.\n",
    "\n",
    "![2-2](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0202.png)\n",
    "\n",
    "Figure 2-2. The quadratic error surface for a linear neuron\n",
    "\n",
    "We can also conveniently visualize this surface as a set of elliptical contours, **where the minimum error is at the center of the ellipses**.\n",
    "\n",
    "In fact, it turns out that **the direction of the steepest descent is always perpendicular to the contours**. This direction is expressed as a vector known as the `gradient`.\n",
    "\n",
    "Suppose we randomly initialize the weights of our network so we find ourselves somewhere on the horizontal plane. By evaluating the gradient at our current position, we can find the direction of steepest descent, and we can take a step in that direction. Then we’ll find ourselves at a new position that’s closer to the minimum than we were before. We can reevaluate the direction of steepest descent by taking the gradient at this new position and taking a step in this new direction. This algorithm is known as `gradient descent`.\n",
    "\n",
    "![2-3](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0203.png)\n",
    "\n",
    "Figure 2-3. Visualizing the error surface as a set of contours\n",
    "\n",
    "## The Delta Rule and Learning Rates\n",
    "![2-4](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0204.png)\n",
    "\n",
    "Figure 2-4. Convergence is difficult when our learning rate is too large\n",
    "\n",
    "Define the learning rate $\\epsilon$, we want:\n",
    "\n",
    "$$\\Delta w_k=-\\epsilon \\frac{\\partial E}{\\partial w_k}=-\\epsilon\\frac{\\partial}{\\partial w_k}(\\frac{1}{2}\\sum_i(t^{(i)}-y^{(i)})^2)=\\sum_i \\epsilon(t^{(i)}-y^{(i)})\\frac{\\partial y_i}{\\partial w_k}=\\sum_i \\epsilon x_k^{(i)}(t^{(i)}-y^{(i)})$$\n",
    "\n",
    "## Gradient Descent with Sigmoidal Neurons\n",
    "$$z=\\sum_k w_k x_k$$\n",
    "\n",
    "$$y=\\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "Taking the derivative of the logit with respect to the inputs and the weights:\n",
    "\n",
    "$$\\frac{\\partial z}{\\partial w_k}=x_k$$\n",
    "\n",
    "$$\\frac{\\partial z}{\\partial x_k}=w_k$$\n",
    "\n",
    "The derivative of the output with respect to the logit:\n",
    "\n",
    "$$\\frac{dy}{dz}=\\frac{e^{-z}}{(1+e^{-z})^2}=\\frac{1}{1+e^{-z}}\\frac{e^{-z}}{1+e^{-z}}=\\frac{1}{1+e^{-z}}(1-\\frac{1}{1+e^{-z}})=y(1-y)$$\n",
    "\n",
    "We then use the chain rule to get the derivative of the output with respect to each weight:\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial w_k}=\\frac{dy}{dz}\\frac{\\partial z}{\\partial w_k}=x_k y(1-y)$$\n",
    "\n",
    "Putting all of this together:\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w_k}=\\sum_i\\frac{\\partial E}{\\partial y^{(i)}}\\frac{\\partial y^{(i)}}{\\partial w_k}=-\\sum_i x_k^{(i)} y^{(i)}(1-y^{(i)})(t^{(i)}-y^{(i)})$$\n",
    "\n",
    "Thus, the final rule for modifying the weights becomes:\n",
    "\n",
    "$$\\Delta w_k=\\sum_i \\epsilon x_k^{(i)}y^{(i)}(1-y^{(i)})(t^{(i)}-y^{(i)})$$\n",
    "\n",
    "## The Backpropagation Algorithm\n",
    "![2-5](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0205.png)\n",
    "\n",
    "Figure 2-5. Reference diagram for the derivation of the backpropagation algorithm\n",
    "\n",
    "- y:refer to the activity of a neuron\n",
    "- z:refer to the logit of the neuron\n",
    "\n",
    "Calculate the error function derivatives at the output layer:\n",
    "\n",
    "$$E=\\frac{1}{2}\\sum_{j \\in output}(t_j-y_j)^2 \\Rightarrow \\frac{\\partial E}{\\partial y_j}=-(t_j-y_j)$$\n",
    "\n",
    "Let’s presume we have the error derivatives for layer j. We now aim to calculate the error derivatives for the layer below it, layer i. To do so, we must accumulate information about how the output of a neuron in layer i affects the logits of every neuron in layer j. This can be done as follows, using the fact that the partial derivative of the logit with respect to the incoming output data from the layer beneath is merely the weight of the connection $w_{ij}$:\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial y_i}=\\sum_j\\frac{\\partial E}{\\partial z_j}\\frac{d z_j}{d y_j}=\\sum_j w_{ij}\\frac{\\partial E}{\\partial z_j}$$\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial z_j}=\\frac{\\partial E}{\\partial y_j}\\frac{d y_j}{d z_j}=y_j (1-y_j) \\frac{\\partial E}{\\partial y_i}$$\n",
    "\n",
    "Combining these two together, we can finally express the error derivatives of layer i in terms of the error derivatives of layer j:\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial y_i}=\\sum_j w_{ij}y_j(1-y_j)\\frac{\\partial E}{\\partial y_j}$$\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w_{ij}}=\\frac{\\partial z_j}{\\partial w_{ij}}\\frac{\\partial E}{\\partial z_j}=y_i y_j(1-y_j)\\frac{\\partial E}{\\partial y_j}$$\n",
    "\n",
    "Finally, we sum up the partial derivatives over all the training examples in our dataset. \n",
    "\n",
    "$$\\Delta w_{ij}=-\\sum_{k \\in dataset} \\epsilon y_i^{(k)}y_j^{(k)}(1-y_j^{(k)})\\frac{\\partial E^{(k)}}{\\partial y_j^{(k)}}$$\n",
    "\n",
    "## Stochastic and Minibatch Gradient Descent\n",
    "![2-6](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0206.png)\n",
    "\n",
    "Figure 2-6. Batch gradient descent is sensitive to saddle points, which can lead to premature convergence\n",
    "\n",
    "SGD:\n",
    "\n",
    "![2-7](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0207.png)\n",
    "\n",
    "Figure 2-7. The stochastic error surface fluctuates with respect to the batch error surface, enabling saddle point avoidance\n",
    "\n",
    "In mini-batch gradient descent, at every iteration, we compute the error surface with respect to some subset of the total dataset (instead of just a single example). This subset is called a `minibatch`, and in addition to the learning rate, minibatch size is another hyperparameter.\n",
    "\n",
    "$$\\Delta w_{ij}=-\\sum_{k \\in minibatch} \\epsilon y_i^{(k)}y_j^{(k)}(1-y_j^{(k)})\\frac{\\partial E^{(k)}}{\\partial y_j^{(k)}}$$\n",
    "\n",
    "## Test Sets, Validation Sets, and Overfitting\n",
    "![2-8](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0208.png)\n",
    "\n",
    "Figure 2-8. Two potential models that might describe our dataset: a linear model versus a degree 12 polynomial\n",
    "\n",
    "![2-9](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0209.png)\n",
    "\n",
    "Figure 2-9. Evaluating our model on new data indicates that the linear fit is a much better model than the degree 12 polynomial\n",
    "\n",
    "![2-10](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0210.png)\n",
    "\n",
    "Figure 2-10. A visualization of neural networks with 3, 6, and 20 neurons (in that order) in their hidden layer\n",
    "\n",
    "![2-11](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0211.png)\n",
    "\n",
    "Figure 2-11. A visualization of neural networks with one, two, and four hidden layers (in that order) of three neurons each\n",
    "\n",
    "This leads to three major observations.First, the machine learning engineer is always working with a direct trade-off between overfitting and model complexity.\n",
    "\n",
    "Second, it is very misleading to evaluate a model using the data we used to train it\n",
    "\n",
    "![2-12](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0212.png)\n",
    "\n",
    "Figure 2-12. We often split our data into nonoverlapping training and test sets in order to fairly evaluate our model\n",
    "\n",
    "Third, it’s quite likely that while we’re training our data, there’s a point in time where instead of learning useful features, we start  overfitting to the training set. To avoid that, we want to be able to stop the training process as soon as we start overfitting, to prevent poor generalization. To do this, we divide our training process into `epochs`. An epoch is a single iteration over the entire training set.\n",
    "\n",
    "At the end of each epoch, we want to measure how well our model is generalizing. To do this, we use an additional `validation set`.\n",
    "\n",
    "![2-13](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0213.png)\n",
    "\n",
    "Figure 2-13. In deep learning we often include a validation set to prevent overfitting during the training process\n",
    "\n",
    "![2-14](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0214.png)\n",
    "\n",
    "Figure 2-14. Detailed workflow for training and evaluating a deep learning model\n",
    "\n",
    "## Preventing Overfitting in Deep Neural Networks\n",
    "`Regularization` modifies the objective function that we minimize by adding additional terms that penalize large weights. In other words, we change the objective function so that it becomes $Error + \\lambda f(\\theta)$, where $f(\\theta)$ grows larger as the components of $\\theta$ grow larger, and $\\lambda$ is the regularization strength (another hyperparameter). \n",
    "\n",
    "The most common type of regularization in machine learning is `L2 regularization`. It can be implemented by augmenting the error function with the squared magnitude of all weights in the neural network. In other words, for every weight w in the neural network, we add $\\frac{1}{2}\\lambda w^2$ to the error function. The L2 regularization has the intuitive interpretation of heavily penalizing peaky weight vectors and preferring diffuse weight vectors. This has the appealing property of encouraging the network to use all of its inputs a little rather than using only some of its inputs a lot. Of particular note is that during the gradient descent update, using the L2 regularization ultimately means that every weight is decayed linearly to zero. Because of this phenomenon, L2 regularization is also commonly referred to as `weight decay`.\n",
    "\n",
    "![2-15](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0215.png)\n",
    "\n",
    "Figure 2-15. A visualization of neural networks trained with regularization strengths of 0.01, 0.1, and 1 (in that order)\n",
    "\n",
    "Another common type of regularization is `L1 regularization`. Here, we add the term $\\lambda|w|$ for every weight w in the neural network. The L1 regularization has the intriguing property that it leads the weight vectors to become sparse during optimization (i.e., very close to exactly zero). In other words, neurons with L1 regularization end up using only a small subset of their most important inputs and become quite resistant to noise in the inputs.  In comparison, weight vectors from L2 regularization are usually diffuse, small numbers. **L1 regularization is very useful when you want to understand exactly which features are contributing to a decision. If this level of feature analysis isn’t necessary, we prefer to use L2 regularization because it empirically performs better.**\n",
    "\n",
    "`Max norm constraints` enforce an absolute upper bound on the magnitude of the incoming weight vector for every neuron and use projected gradient descent to enforce the constraint. In other words, any time a gradient descent step moves the incoming weight vector such that $\\lVert w \\rVert^2 > c$, we project the vector back onto the ball (centered at the origin) with radius c.\n",
    "\n",
    "While training, `dropout` is implemented by only keeping a neuron active with some probability p (a hyperparameter), or setting it to zero otherwise. Intuitively, this forces the network to be accurate even in the absence of certain information. It prevents the network from becoming too dependent on any one (or any small combination) of neurons. Expressed more mathematically, it prevents overfitting by providing a way of approximately combining exponentially many different neural network architectures efficiently.\n",
    "\n",
    "![2-16](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0216.png)\n",
    "\n",
    "Figure 2-16. Dropout sets each neuron in the network as inactive with some random probability during each minibatch of training"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

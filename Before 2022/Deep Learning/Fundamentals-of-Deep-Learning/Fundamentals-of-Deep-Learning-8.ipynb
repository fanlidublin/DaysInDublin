{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals of Deep Learning\n",
    "## 目录\n",
    "- Chapter 8. Memory Augmented Neural Networks\n",
    "    - Neural Turing Machines\n",
    "    - Attention-Based Memory Access\n",
    "    - Differentiable Neural Computers\n",
    "    - Interference-Free Writing in DNCs\n",
    "    - DNC Memory Reuse\n",
    "    - Temporal Linking of DNC Writes\n",
    "    - Understanding the DNC Read Head\n",
    "    - The DNC Controller Network\n",
    "\n",
    "## Neural Turing Machines\n",
    "[Neural Turing Machines,2014, Graves et al.](https://arxiv.org/abs/1410.5401):\n",
    "\n",
    "![8-1](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0801.png)\n",
    "\n",
    "Figure 8-1. Comparing the architecture of a modern day computer which is fed its program (left) to a Neural Turing Machine that learns its program (right). This example has a single read head and single write head, but an NTM can have several in practice. \n",
    "\n",
    "## Attention-Based Memory Access\n",
    "![8-2](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0802a.png)\n",
    "\n",
    "Figure 8-2. A demonstration of how a blurry attention-based reading can retrieve a vector containing approximately the same information as in the focused-on location\n",
    "\n",
    "## Differentiable Neural Computers\n",
    "Despite the power of NTMs, they have a few limitations regarding their memory mechanisms. The first of these limitations is that NTMs have no way to ensure that no interference or overlap between written data would occur.\n",
    "\n",
    "However, even when the NTM converges to an interference-free behavior, once a memory location has been written to, there’s no way to reuse that location again, even when the data stored in it becomes irrelevant. The inability to free and reuse memory locations is the second limitation to the NTM architecture.\n",
    "\n",
    "In October 2016, Graves et al. from DeepMind published in Nature a paper titled [“Hybrid computing using a neural network with dynamic external memory”](http://go.nature.com/2peM8m2) in which they introduced a new memory-augmented neural architecture called **differentiable neural computer**  (DNC) that improves on NTMs and addresses those limitations we just discussed.\n",
    "\n",
    "Figure 8-6 summarizes the operation of the DNC that we just described.  We can see that unlike NTMs, DNCs keep other data structures alongside the memory itself to keep track of the state of the memory. \n",
    "\n",
    "![8-6](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0806.png)\n",
    "\n",
    "Figure 8-6. An overview of DNC’s architecture and operation.\n",
    "DNC’s external memory differs from that of an NTM by several extra data structures as well as by the attention mechanisms used to access the memory.\n",
    "\n",
    "## Interference-Free Writing in DNCs\n",
    "The first limitation we discussed of NTMs was their inability to ensure an interference-free writing behavior. An intuitive way to address this issue is to design the architecture to focus strongly on a single, free memory location and not wait for NTM to learn to do so. In order to keep track of which locations are free and which are busy, we need to introduce a new data structure that can hold this kind of information. We’ll call it the **usage vector**.\n",
    "\n",
    "## DNC Memory Reuse\n",
    "In order to know which locations can be freed and which cannot, we construct a retention vector $\\psi_t$ of size N that specifies how much of each location should be retained and not get freed. Each element of this vector takes a value between 0 and 1, with 0 indicating that the corresponding location can be freed and 1 indicating that it should be retained. \n",
    "\n",
    "## Temporal Linking of DNC Writes\n",
    "With the dynamic memory management mechanisms that DNCs use, each time a memory location is requested for allocation, we’re going to get the most unused location, and there’ll be no positional relation between that location and the location of the previous write.\n",
    "\n",
    "## Understanding the DNC Read Head\n",
    "Once the write head has finished updating the memory matrix and the associated data structures, the read head is now ready to work. Its operation is simple: it needs to be able to look up values in the memory and be able to iterate forward and backward in temporal ordering between data. \n",
    "\n",
    "## The DNC Controller Network\n",
    "The controller’s operation is simple: in its heart there’s a neural network (recurrent or feed-forward) that takes in the input step along with the read-vectors from the last step and outputs a vector whose size depends on the architecture we chose for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
